The first step of MBO requires an initial set of evaluation points which is then evaluated by the black box function.
The basic procedure of MBO is an iterating loop of the following steps:
Firstly, a user defined surrogate model is fitted on the evaluated points, secondly, a new evaluation point is proposed by an infill criterion and lastly, its performance is evaluated.
The result of this sequential procedure is the optimization path containing the best parameter setting and the fitted surrogate model.

Once again, take a look at the already introduced basic example of the optimization of the one dimensional Rastrigin function.

```{r eval=FALSE}
library(mlrMBO)
obj.fun = makeRastriginFunction(1)

learner = makeLearner("regr.km", predict.type = "se", covtype = "matern3_2")
control = makeMBOControl()
control = setMBOControlTermination(control, iters = 5)
control = setMBOControlInfill(control, crit = "ei")

result = mbo(obj.fun, learner = learner, control = control, show.info = TRUE)
```

From this example we can easily recognize some **mlrMBO** essentials like parameters, learners and the control object.

Basically the following steps are needed to start a surrogate-based optimization with our package. 
Each step ends with an R object, which is then passed to ```mbo()```, i. e., to the working horse of mlrMBO.

1. define the objective function and its parameters by using the package **smoof**
1. (optionally generate an initial design)
1. define a learner, i. e., the surrogate model
1. set up a MBO control object, which offers a load of options
1. finally start the optimization

This web page will provide you with an in-depth introduction on how to set the ``mbo()`` parameters depending on the desired kind of optimization.


# Objective Function


The first argument of ``mbo()`` is the name of the objective function created with the package **smoof**. 
The first argument of this objective function has to be a list of values.
The function has to return a single numerical value. 
We demonstrate in this tutorial the optimization of two simple functions: The 5 dimensional ``ackley function`` (``objfun1``) and a self-constructed sine und cosine combination (``objfun2``), where we do not minimize but maximize the function. 
``objfun1`` depends on 5 numeric parameters, while ``objfun2`` assumes 2 numeric and 1 discrete parameters.

The 5 dimensional ``ackley function`` can be get by using the appropriate function of the **smoof** package

```{r}
objfun1 = makeAckleyFunction(5) 
```

The self-constructed function is built with ``makeSingleObjetiveFunction``. 
It has the attribute ``par.set`` that has to be a ParamSet object from the **ParamHelpers** package, which provides information about the parameters of the objective function and their constraints for optimization.
We assume ``x`` from interval [0,1] and ``k`` from interval [1,2]. Parameter ``method`` can be either ``"a"`` or ``"b"``.
The type of optimization, in this case maximization, is also defined in ``makeSingleObjetiveFunction`` with ``minimize = TRUE`` as default.

```{r}
foo = function(listOfValues) {
  x = listOfValues[[1]]
  k = listOfValues[[2]]
  method = listOfValues[[3]]
  perf = ifelse(method == "a", k * sin(x) + cos(x),
               sin(x) + k * cos(x))
  return(perf)
}

objfun2 = makeSingleObjectiveFunction(
  name = "example",
  fn = foo,
  par.set = makeParamSet(
    makeNumericParam("x", lower = 0,upper = 1),
    makeIntegerParam("k", lower = 1, upper = 2),
    makeDiscreteParam("method", values = c("a", "b"))
  ),
  has.simple.signature = FALSE,
  minimize = FALSE
)
```


# Initial Design


The second argument of the ``mbo()`` function - ``design`` - is the initial design with default setting ``NULL``.

It is recommendable to use the ``generateDesign`` function from **ParamHelpers** package to create it.
However, if special designs are desired (e.g., orthogonal designs), its interface has to be the same as the interface of the ``generateDesign`` objects. 
Particular attention has to be paid to the setting of the ``trafo`` attribute.

```{r}
init.points1 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun1)))
# NOTE:
# init.points1 = 5 * getParamLengths(getParamSet(objfun1))
# is not working, because mlr::getParamSet() is used by default.
init.fun1 = randomLHS
set.seed(1)
design1 = generateDesign(n = init.points1, par.set = smoof::getParamSet(objfun1), fun = init.fun1, trafo = FALSE)

init.points2 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun2)))
init.fun2 = maximinLHS
init.args2 = list(k = 3, dup = 4)
design2 = generateDesign(n = init.points2, par.set = smoof::getParamSet(objfun2), fun.args = init.args2, fun = init.fun2, trafo = FALSE)
```


# Surrogate Model


Attribute ``learner`` of the ``mbo()`` function allows us to choose an appropriate surrogate model for the parameter optimization.
It can be easily done using the ``makeLearner`` function from **mlr** package.
List of implemented learners can be seen using the ?learners command or on ``http://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/``
<!-- NEW -->
The choice of the surrogate model depends on the parameter set of the objective function.
While kriging models are advisable for the numeric parameters, random forest models can be used if at least one parameter is factorial.
In our example we consider these two surrogate models:
``kriging`` for optimizing of ``objfun1``  and ``random forest`` for ``objfun2``.

```{r}
learner_km = makeLearner("regr.km", predict.type = "se", covtype = "matern3_2")
learner_rf = makeLearner("regr.randomForest")
```

However, in some cases it is necessary to modify the learners (e.g., in order to get the standard error prediction for design points).
This will be discussed und illustrated in the section "Experiments and Output".


# MBOControl


The ``MBOControl`` object can be created with ``makeMBOControl`` and some attributes can be set when creating it.
But there are also further functions to define or change the settings of the object:  


``setMBOControlInfill``
=======================

Attribute ``crit``
------------------
One of the most important issues is to define how the next design points in the sequential loops have to be chosen. 
Firstly, we have to choose the infill criterion using the  ``crit`` attribute of ``setMBOControlInfill``.
At the moment five possibilities are implemented:
* ``mean``: mean response of the surrogate model,
* ``ei``: expected improvement of the surrogate model,
* ``aei``: augmented expected improvement, which is especially useful for the noisy functions,
* ``eqi``: expected quantile improvement
* ``cb``: confidence bound which is the additive combination of mean response and mean standard error estimation of the surrogate model (response - lambda * standard.error). 
The default value of lambda is 1, but it can easily be changed by
the ``crit.cb.lambda`` attribute.


Attribute ``opt``
-----------------

The attribute ``opt`` sets how the next point to evaluate should be proposed given an infill criterion. 
The possibilities are ``focussearch``, ``cmaes``, ``ea`` and ``nsga2``.

If ``focussearch`` is chosen, the common procedure is as follows: 
in the first step an lhs design is sampled in the parameter space (by ``randomLHS`` function) and the design point with the best  prediction of the infill criterion is determined. 
User can set the size of this design by the ``opt.focussearch.points`` attribute (default value is 10000). 
In the second step the parameter space is shrunk around the best design point in a certain way which should not be discussed in detail here. 
First and second steps are repeated iteratively ``opt.focussearch.maxit`` times (default is 5) while the best seen value of the infill criterion is passed back.

If ``opt`` is ``cmaes``, the point, which optimizes the
infill criterion, is chosen via ``cma_es`` function of  **cmaes** package. Control argument for ``cmaes`` optimizer can be provided in
the ``opt.cmaes.control`` attribute (default is empty list).


If ``opt`` is ``ea`` a simple (mu+1)-evolutionary optimization algorithm is used to optimize the infill criterion.
The population size mu can be set by the ``opt.ea.mu`` attribute (default value is 10).
(mu+1) means that in each population only one child is generated using crossover und mutation operators (from **emao** package).

The parameters ``eta`` and ``p`` of the latter two operators can be adjusted via the attributes ``opt.ea.sbx.eta``, ``opt.ea.sbx.p``,
``opt.ea.pm.eta`` and ``opt.ea.pm.p``.
The default number of EA iterations is 500 and can be changed by ``opt.ea.maxit`` attribute.

For multi objective optimization, ``opt`` should be set to ``nsga2``. 
This is needed for mspot.
<!-- FIXME informations about "nsga2" -->

As all four infill optimization strategies do not guarantee  to find the global optimum, users can set the number of restarts by the ``opt.restarts`` attribute (default value is 1).
After conducting the desired number of restarts the design point with the best infill criterion value is passed back to the MBO loop.

Please note that just ``focussearch`` optimizer is suitable for the case of factor parameters in the parameter set!


Further functions
=================

The number of sequential steps (iterations) can be set via the attribute ``iters`` of the function ``setMBOControlTermination`` (default setting is 10).

There are also many other functions and attributes which user can set in a desired way...
<!-- FIXME further informations about setMBOControl... functions here ? -->
<!-- like how often should the surrogate model be stored or resampled during the optimization. -->
The list of all attributes is provided in the software documentation.
<!-- Let us construct ``mboControl`` objects for our two object functions. -->

```{r eval=FALSE}
control1 = makeMBOControl()
control1 = setMBOControlInfill(
  control = control1,
  crit = "ei",
  opt = "cmaes"
)
control1 = setMBOControlTermination(
  control = control1,
  iters = 10
)

control2 = makeMBOControl()
control2 = setMBOControlInfill(
  control = control2,
  crit = "mean",
  opt = "focussearch"
)
control2 = setMBOControlTermination(
  control = control2,
  iters = 10
)
```

Experiments and Output
======================

Now we will apply the mbo() function to optimize the two objective functions.

Optimization of ``objfun1``
---------------------------


<!-- FIXME??? Optimization has a long computation time -->
```{r eval=FALSE}
mbo1 = mbo(objfun1, design = design1, learner = learner_km, control = control1, show.info = TRUE)
mbo1
getOptPathY(mbo1$opt.path, "y") # get all y values

```

The output of mbo function is a structure of several variables. The most important are:

* x: the best point of the parameter space
* y: the associated best value of the objective function
* opt.path: optimization path. See **ParamHelpers** for further information.
* models: If no other setting were provided in the ``MBOControl`` object, the last estimated surrogate is given here.
* ...


<!-- FIXME: get optimization path as data.frame !-->

We can also change some attributes of the ``MBOControl`` object and run mbo() function again

```{r eval=FALSE}
control1$infill.crit = "mean"
control1$infill.opt = "focussearch"
mbo1 = mbo(objfun1, design = design1, learner = learner_km, control = control1, show.info = FALSE)
mbo1$y
```


Optimization of ``objfun2``
---------------------------

Let us apply **mlrMBO** package to optimize object2 function, which contains one factor variable.
We have already mentioned before that in this case just the ``focussearch`` infill optimization function is suitable.
If we use ``mean`` as infill criterion any kind of model which can handle with factor variables can be used here (like random tree, random forest, linear model and many others).

```{r eval=FALSE}
mbo2 = mbo(objfun2, design = design2, learner = learner_rf, control = control2, show.info = FALSE)
mbo2$y
```

In contrast, if one will apply ``ei`` or ``cb`` infill criteria,
the ``predict.type`` attribute of the learner have be set to ``se``, if possible. A list of regression learners which support it can be viewed by:

```{r eval=FALSE}
#listLearners(type = "regr", se = TRUE)
```

<!-- If no comment here, we get a lot warning message !-->

We hence modify the random forest learner and optimize ``objfun2`` by ``ei`` infill criterion.

```{r eval=FALSE}
learner_rf = makeLearner("regr.randomForest", predict.type = "se")
control2$infill.crit = "ei"
mbo2 = mbo(objfun2, design = design2, learner = learner_rf, control = control2, show.info = FALSE)
mbo2$y
```


Finally, if a learner, which does not support the ``se`` prediction type, should be applied for the optimization with ``ei`` infill criterion, there is a possibility to create a bagging model with the desired characteristics. 
The attribute ``bw.iters`` provides the number of models in the ensemble, see documentation for ``makeBaggingWrapper`` of **mlr** package.


```{r eval=FALSE}
learner_rt = makeLearner("regr.rpart")
bag_rt = makeBaggingWrapper(learner_rt, bw.iters = 5)
setPredictType(bag_rt, predict.type = "se")
mbo2 = mbo(objfun2, design = design2, learner = bag_rt, control = control2, show.info = FALSE)
mbo2$y
```

<!--
 TODO

1) noisy optimization example

2) mulicrit

3) multipoint
!-->
