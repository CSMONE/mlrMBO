<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../img/favicon.ico">

	<title>In Depth Introduction - mlrMBO tutorial</title>

        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../css/highlight.css">
        <link href="../css/custom_mlr.css" rel="stylesheet">
        <link href="../css/custom_highlight.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->
            <a class="navbar-brand" href="../index.html">mlrMBO tutorial</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="../index.html">Home</a>
                    </li>
                
                
                
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Basics <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../quickstart/index.html">Quickstart</a>
</li>

                        
                            
<li class="active">
    <a href="index.html">In Depth Introduction</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Advanced <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../parallelization/index.html">Parallelization</a>
</li>

                        
                            
<li >
    <a href="../multi_point_proposal/index.html">Multi point proposal</a>
</li>

                        
                            
<li >
    <a href="../multi_criteria_optimization/index.html">Multi-criteria optimization</a>
</li>

                        
                        </ul>
                    </li>
                
                
                </ul>
            

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                
                    <li >
                        <a rel="next" href="../quickstart/index.html">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../parallelization/index.html">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
                
                
                    <li>
                        <a href="https://github.com/mlr-org/mlrMBO/">
                            
                                <i class="fa fa-github"></i>
                            
                            GitHub
                        </a>
                    </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#introduction">Introduction</a></li>
        
    
        <li class="main "><a href="#objective-function">Objective Function</a></li>
        
    
        <li class="main "><a href="#initial-design">Initial Design</a></li>
        
    
        <li class="main "><a href="#surrogate-model">Surrogate Model</a></li>
        
    
        <li class="main "><a href="#mbocontrol">MBOControl</a></li>
        
            <li><a href="#mbocontrolinfill">MBOControlInfill</a></li>
        
            <li><a href="#setmbocontroltermination">setMBOControlTermination</a></li>
        
            <li><a href="#setmbocontrolmultipoint">setMBOControlMultiPoint</a></li>
        
            <li><a href="#setmbocontrolmultifid">setMBOControlMultiFid</a></li>
        
            <li><a href="#setmbocontrolmulticrit">setMBOControlMultiCrit</a></li>
        
    
        <li class="main "><a href="#experiments-and-output">Experiments and Output</a></li>
        
            <li><a href="#optimization-of-objfun1">Optimization of objfun1</a></li>
        
            <li><a href="#optimization-of-objfun2">Optimization of objfun2</a></li>
        
    
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="introduction">Introduction</h1>
<p>The first step of MBO requires an initial set of points which are then evaluated by the black box function.</p>
<p>The procedure of MBO is a loop of the following steps:</p>
<ol>
<li>A user defined surrogate model is fitted on the evaluated points</li>
<li>A new evaluation point is proposed by an infill criterion </li>
<li>Its performance is evaluated</li>
</ol>
<p>We take a look at the already <a href="../quickstart/index.html">quickstart</a> example of optimizing the one dimensional Rastrigin function.</p>
<pre><code class="r">library(mlrMBO)
#&gt; Loading required package: lhs
#&gt; Loading required package: smoof
#&gt; Loading required package: checkmate
#&gt; 
#&gt; Attaching package: 'smoof'
#&gt; The following object is masked from 'package:mlr':
#&gt; 
#&gt;     getParamSet
#&gt; Warning: replacing previous import by 'smoof::getParamSet' when loading
#&gt; 'mlrMBO'
#&gt; 
#&gt; Attaching package: 'mlrMBO'
#&gt; The following object is masked from 'package:ParamHelpers':
#&gt; 
#&gt;     plotEAF
obj.fun = makeRastriginFunction(1)

learner = makeLearner(&quot;regr.km&quot;, predict.type = &quot;se&quot;, covtype = &quot;matern3_2&quot;)
control = makeMBOControl()
control = setMBOControlTermination(control, iters = 5)
control = setMBOControlInfill(control, crit = &quot;ei&quot;, opt = &quot;ea&quot;)

result = mbo(obj.fun, learner = learner, control = control, show.info = TRUE)
#&gt; Computing y column(s) for design. Not provided.
#&gt; [mbo] 0: x=3.29 : y = 23.5 : 0.0 secs : initdesign
#&gt; [mbo] 0: x=-3.22 : y = 18.5 : 0.0 secs : initdesign
#&gt; [mbo] 0: x=-0.777 : y = 8.9 : 0.0 secs : initdesign
#&gt; [mbo] 0: x=1.93 : y = 4.59 : 0.0 secs : initdesign
#&gt; 
#&gt; optimisation start
#&gt; ------------------
#&gt; * estimation method   : MLE 
#&gt; * optimisation method : BFGS 
#&gt; * analytical gradient : used
#&gt; * trend model : ~1
#&gt; * covariance model : 
#&gt;   - type :  matern3_2 
#&gt;   - nugget : NO
#&gt;   - parameters lower bounds :  1e-10 
#&gt;   - parameters upper bounds :  13.02456 
#&gt;   - best initial criterion value(s) :  -14.00775 
#&gt; 
#&gt; N = 1, M = 5 machine precision = 2.22045e-16
#&gt; At X0, 0 variables are exactly at the bounds
#&gt; At iterate     0  f=       14.008  |proj g|=      0.72708
#&gt; At iterate     1  f =       13.729  |proj g|=             0
#&gt; 
#&gt; iterations 1
#&gt; function evaluations 2
#&gt; segments explored during Cauchy searches 1
#&gt; BFGS updates skipped 0
#&gt; active bounds at final generalized Cauchy point 1
#&gt; norm of the final projected gradient 0
#&gt; final function value 13.7295
#&gt; 
#&gt; F = 13.7295
#&gt; final  value 13.729484 
#&gt; converged
#&gt; Loading required package: emoa
#&gt; 
#&gt; Attaching package: 'emoa'
#&gt; The following object is masked from 'package:BBmisc':
#&gt; 
#&gt;     coalesce
#&gt; [mbo] 1: x=3.5 : y = 32.2 : 0.0 secs : infill_ei
#&gt; 
#&gt; optimisation start
#&gt; ------------------
#&gt; * estimation method   : MLE 
#&gt; * optimisation method : BFGS 
#&gt; * analytical gradient : used
#&gt; * trend model : ~1
#&gt; * covariance model : 
#&gt;   - type :  matern3_2 
#&gt;   - nugget : NO
#&gt;   - parameters lower bounds :  1e-10 
#&gt;   - parameters upper bounds :  13.4358 
#&gt;   - best initial criterion value(s) :  -18.15923 
#&gt; 
#&gt; N = 1, M = 5 machine precision = 2.22045e-16
#&gt; At X0, 0 variables are exactly at the bounds
#&gt; At iterate     0  f=       18.159  |proj g|=      0.24804
#&gt; At iterate     1  f =       18.157  |proj g|=      0.019406
#&gt; At iterate     2  f =       18.157  |proj g|=     0.0010016
#&gt; At iterate     3  f =       18.157  |proj g|=    4.3897e-06
#&gt; 
#&gt; iterations 3
#&gt; function evaluations 5
#&gt; segments explored during Cauchy searches 3
#&gt; BFGS updates skipped 0
#&gt; active bounds at final generalized Cauchy point 0
#&gt; norm of the final projected gradient 4.3897e-06
#&gt; final function value 18.157
#&gt; 
#&gt; F = 18.157
#&gt; final  value 18.156974 
#&gt; converged
#&gt; [mbo] 2: x=1.76 : y = 12.7 : 0.0 secs : infill_ei
#&gt; 
#&gt; optimisation start
#&gt; ------------------
#&gt; * estimation method   : MLE 
#&gt; * optimisation method : BFGS 
#&gt; * analytical gradient : used
#&gt; * trend model : ~1
#&gt; * covariance model : 
#&gt;   - type :  matern3_2 
#&gt;   - nugget : NO
#&gt;   - parameters lower bounds :  1e-10 
#&gt;   - parameters upper bounds :  13.4358 
#&gt;   - best initial criterion value(s) :  -21.29922 
#&gt; 
#&gt; N = 1, M = 5 machine precision = 2.22045e-16
#&gt; At X0, 0 variables are exactly at the bounds
#&gt; At iterate     0  f=       21.299  |proj g|=      0.33594
#&gt; At iterate     1  f =       21.239  |proj g|=        0.2966
#&gt; At iterate     2  f =       21.215  |proj g|=       0.76838
#&gt; At iterate     3  f =       21.206  |proj g|=       0.14353
#&gt; At iterate     4  f =       21.206  |proj g|=      0.013429
#&gt; At iterate     5  f =       21.206  |proj g|=    0.00027265
#&gt; At iterate     6  f =       21.206  |proj g|=    5.0258e-07
#&gt; 
#&gt; iterations 6
#&gt; function evaluations 8
#&gt; segments explored during Cauchy searches 6
#&gt; BFGS updates skipped 0
#&gt; active bounds at final generalized Cauchy point 0
#&gt; norm of the final projected gradient 5.02585e-07
#&gt; final function value 21.2059
#&gt; 
#&gt; F = 21.2059
#&gt; final  value 21.205949 
#&gt; converged
#&gt; [mbo] 3: x=2.06 : y = 4.87 : 0.0 secs : infill_ei
#&gt; 
#&gt; optimisation start
#&gt; ------------------
#&gt; * estimation method   : MLE 
#&gt; * optimisation method : BFGS 
#&gt; * analytical gradient : used
#&gt; * trend model : ~1
#&gt; * covariance model : 
#&gt;   - type :  matern3_2 
#&gt;   - nugget : NO
#&gt;   - parameters lower bounds :  1e-10 
#&gt;   - parameters upper bounds :  13.4358 
#&gt;   - best initial criterion value(s) :  -28.29369 
#&gt; 
#&gt; N = 1, M = 5 machine precision = 2.22045e-16
#&gt; At X0, 0 variables are exactly at the bounds
#&gt; At iterate     0  f=       28.294  |proj g|=       1.6315
#&gt; At iterate     1  f =       25.707  |proj g|=             0
#&gt; 
#&gt; iterations 1
#&gt; function evaluations 2
#&gt; segments explored during Cauchy searches 1
#&gt; BFGS updates skipped 0
#&gt; active bounds at final generalized Cauchy point 1
#&gt; norm of the final projected gradient 0
#&gt; final function value 25.7075
#&gt; 
#&gt; F = 25.7075
#&gt; final  value 25.707464 
#&gt; converged
#&gt; [mbo] 4: x=1.49 : y = 22.2 : 0.0 secs : infill_ei
#&gt; 
#&gt; optimisation start
#&gt; ------------------
#&gt; * estimation method   : MLE 
#&gt; * optimisation method : BFGS 
#&gt; * analytical gradient : used
#&gt; * trend model : ~1
#&gt; * covariance model : 
#&gt;   - type :  matern3_2 
#&gt;   - nugget : NO
#&gt;   - parameters lower bounds :  1e-10 
#&gt;   - parameters upper bounds :  13.4358 
#&gt;   - best initial criterion value(s) :  -26.77505 
#&gt; 
#&gt; N = 1, M = 5 machine precision = 2.22045e-16
#&gt; At X0, 0 variables are exactly at the bounds
#&gt; At iterate     0  f=       26.775  |proj g|=      0.24803
#&gt; At iterate     1  f =       26.774  |proj g|=      0.020612
#&gt; At iterate     2  f =       26.774  |proj g|=    0.00051553
#&gt; At iterate     3  f =       26.774  |proj g|=      1.11e-06
#&gt; 
#&gt; iterations 3
#&gt; function evaluations 5
#&gt; segments explored during Cauchy searches 3
#&gt; BFGS updates skipped 0
#&gt; active bounds at final generalized Cauchy point 0
#&gt; norm of the final projected gradient 1.10996e-06
#&gt; final function value 26.7739
#&gt; 
#&gt; F = 26.7739
#&gt; final  value 26.773908 
#&gt; converged
#&gt; [mbo] 5: x=-1.13 : y = 4.61 : 0.0 secs : infill_ei
#&gt; 
#&gt; optimisation start
#&gt; ------------------
#&gt; * estimation method   : MLE 
#&gt; * optimisation method : BFGS 
#&gt; * analytical gradient : used
#&gt; * trend model : ~1
#&gt; * covariance model : 
#&gt;   - type :  matern3_2 
#&gt;   - nugget : NO
#&gt;   - parameters lower bounds :  1e-10 
#&gt;   - parameters upper bounds :  13.4358 
#&gt;   - best initial criterion value(s) :  -30.8632 
#&gt; 
#&gt; N = 1, M = 5 machine precision = 2.22045e-16
#&gt; At X0, 0 variables are exactly at the bounds
#&gt; At iterate     0  f=       30.863  |proj g|=       7.3933
#&gt; At iterate     1  f =       30.725  |proj g|=       0.64711
#&gt; At iterate     2  f =       30.376  |proj g|=       0.51843
#&gt; At iterate     3  f =       30.264  |proj g|=       0.11344
#&gt; At iterate     4  f =       30.264  |proj g|=      0.034592
#&gt; At iterate     5  f =       30.264  |proj g|=     0.0004087
#&gt; At iterate     6  f =       30.264  |proj g|=    1.4475e-06
#&gt; 
#&gt; iterations 6
#&gt; function evaluations 10
#&gt; segments explored during Cauchy searches 6
#&gt; BFGS updates skipped 0
#&gt; active bounds at final generalized Cauchy point 0
#&gt; norm of the final projected gradient 1.44747e-06
#&gt; final function value 30.2635
#&gt; 
#&gt; F = 30.2635
#&gt; final  value 30.263536 
#&gt; converged
</code></pre>

<p>From this example we can easily see some <strong>mlrMBO</strong> essentials, like parameters, learners and the control object.</p>
<p>Basically the following steps are needed to start a surrogate-based optimization with our package. 
Each step ends with an R object, which is then passed to <code>mbo()</code>, i.e., to the working horse of mlrMBO.</p>
<ol>
<li>define the objective function and its parameters by using the package <strong>smoof</strong></li>
<li>generate an initial design</li>
<li>define a learner, i.e., the surrogate model</li>
<li>set up a MBO control object</li>
<li>finally start the optimization</li>
</ol>
<p>This web page will provide you with an in-depth introduction on how to set the <code>mbo()</code> parameters for different kind of optimizations.</p>
<h1 id="objective-function">Objective Function</h1>
<p>The first argument of <code>mbo()</code> is the the objective function created with <code>makeSingleObjectiveFunction</code> (or <code>makeMultiObjectiveFunction</code>) from the package <strong>smoof</strong>. </p>
<p>Throughout this tutorial we demonstrate the optimization of two simple functions: </p>
<ul>
<li>
<p><code>objfun1</code>: The 5 dimensional <code>ackley function</code>, which depends on 5 numeric parameters. <code>objfun1</code> should be minimized.</p>
</li>
<li>
<p><code>objfun2</code>: A self-constructed sine und cosine combination, with two numeric and 1 categorical parameters. <code>objfun2</code> should be maximized. </p>
</li>
</ul>
<p>The 5 dimensional <code>ackley function</code> can be generated by the appropriate function of the <strong>smoof</strong> package</p>
<pre><code class="r">objfun1 = makeAckleyFunction(5)
objfun1(c(1.8, 2.2, -4, 4, -5))
#&gt; [1] 10.9365
</code></pre>

<p>The self-constructed function can be built with <code>makeSingleObjetiveFunction</code>. The <code>par.set</code> argument has to be a ParamSet object from the <strong>ParamHelpers</strong> package, which provides information about the parameters of the objective function and their constraints for optimization.
We define <code>j</code> in the interval [0,1] and <code>k</code> as an integer in {1, 2}. The Parameter <code>method</code> is categorical and can be either <code>"a"</code> or <code>"b"</code>.
As stated, in this case we want to maximize the function. To do so we have to set <code>minimize = FALSE</code>.</p>
<pre><code class="r">foo = function(x) {
  j = x[[1]]
  k = x[[2]]
  method = x[[3]]
  perf = ifelse(method == &quot;a&quot;, k * sin(j) + cos(j),
               sin(j) + k * cos(j))
  return(perf)
}

objfun2 = makeSingleObjectiveFunction(
  name = &quot;example&quot;,
  fn = foo,
  par.set = makeParamSet(
    makeNumericParam(&quot;j&quot;, lower = 0,upper = 1),
    makeIntegerParam(&quot;k&quot;, lower = 1L, upper = 2L),
    makeDiscreteParam(&quot;method&quot;, values = c(&quot;a&quot;, &quot;b&quot;))
  ),
  has.simple.signature = FALSE,
  minimize = FALSE
)

objfun2(list(j = 0.5, k = 1L, method = &quot;a&quot;))
#&gt; [1] 1.357008
</code></pre>

<h1 id="initial-design">Initial Design</h1>
<p>The second argument of the <code>mbo()</code> function - <code>design</code> - is the initial design with default setting <code>NULL</code>.</p>
<p>An easy (and recommended) way to create an initial design is to use the <code>generateDesign</code> function from the <strong>ParamHelpers</strong> package. If the default settings are used (i.e. <code>design = NULL</code>) a Random Latin Hypercube <code>lhs::randomLHS</code> design is used with 4 times the number of parameters the objective function has. Other possibilities to generate designs are for example <code>generateGridDesign</code> and <code>generateRandomDesign</code>. </p>
<p><em>Note:</em> If special designs are desired (e.g., orthogonal designs), its interface has to be the same as the interface of the <code>generateDesign</code> objects. </p>
<p>For <code>objfun1</code> and <code>objfun2</code> we create a slightly larger number of initial points than the default suggests. For <code>objfun1</code> we use Random Latin Hypercube sampling and for <code>objfun2</code> the Maximin Latin Hypercube sampling. The parameters of the sampling design have to be specified in a list and supplied via <code>fun.args</code>.</p>
<pre><code class="r">init.points1 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun1)))

set.seed(1)
design1 = generateDesign(n = init.points1, par.set = smoof::getParamSet(objfun1), fun = randomLHS, trafo = FALSE)

init.points2 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun2)))
design2 = generateDesign(n = init.points2, par.set = smoof::getParamSet(objfun2), fun.args = list(k = 3, dup = 4), fun = maximinLHS, trafo = FALSE)
</code></pre>

<h1 id="surrogate-model">Surrogate Model</h1>
<p>The attribute <code>learner</code> of the <code>mbo()</code> function allows us to choose an appropriate surrogate model for the parameter optimization. Different learners can easily created using the <code>makeLearner</code> function from the <strong>mlr</strong> package.
List of implemented learners can be seen using the <code>listlearners()</code> function or on the <a href="http://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/">mlr wiki</a>.</p>
<p>The choice of the surrogate model depends on the parameter set of the objective function.
While kriging models (gaussian processes) are advisable if all parameters are numeric, they cannot be used if the objective function contains categorical parameters. If at least one parameter is categorical, random forest models can be used as surrogate models. The default kriging model is from the <strong>DiceKriging</strong> package and uses the <code>matern5_2</code>covariance kernel.
In our example we consider these two surrogate models:
<code>kriging</code> for optimizing of <code>objfun1</code> and <code>random forest</code> for <code>objfun2</code>.</p>
<pre><code class="r">surr.km = makeLearner(&quot;regr.km&quot;, predict.type = &quot;se&quot;, covtype = &quot;matern3_2&quot;)
surr.rf = makeLearner(&quot;regr.randomForest&quot;)
</code></pre>

<p>Further modification of the learner (e.g., in order to get standard error prediction for design points) will be discussed und illustrated in the section <em>Experiments and Output</em>.</p>
<h1 id="mbocontrol">MBOControl</h1>
<p>The <code>MBOControl</code> object controls the fitting process and is created with <code>makeMBOControl</code>. General control arguments can be set when creating it (e.g. the number of objectives (<code>n.objectives</code>), the number of points to propose in each iteration (<code>propose.points</code>), how the final point is proposed (<code>final.method</code>) etc.).
To further adapt the optimization, additional control functions are used to define or change settings of the object:  </p>
<h2 id="mbocontrolinfill">MBOControlInfill</h2>
<p>With <code>setMBOControlInfill</code> a <code>MBOControl</code> object can be extended with infill criteria and infill optimizer options.</p>
<h3 id="argument-crit">Argument <em>crit</em></h3>
<p>One of the most important questions is to define how the next design points in the sequential loop are chosen. 
5 different possibilities can be set via the <code>crit</code> argument in <code>setMBOControlInfill</code>:</p>
<ul>
<li><code>mean</code>: mean response of the surrogate model</li>
<li><code>ei</code>: expected improvement of the surrogate model</li>
<li><code>aei</code>: augmented expected improvement, which is especially useful for the noisy functions</li>
<li><code>eqi</code>: expected quantile improvement</li>
<li><code>cb</code>: confidence bound, which is the additive combination of mean response and mean standard error estimation of the surrogate model (response - lambda * standard.error)</li>
</ul>
<p>The parameters of the different criteria are set via further arguments (e.g. <code>crit.cb.lambds</code> for the lambda parameter if <code>crit = cb</code>) </p>
<h3 id="argument-opt">Argument <em>opt</em></h3>
<p>The argument <code>opt</code> sets how the next point to evaluate should be proposed given an infill criterion. 
The possibilities are:</p>
<ul>
<li><code>focussearch</code>: Firstly a Latin Hypercube design of size <code>opt.focussearch.points</code> (default 10000) is sampled in the parameter space (by <code>randomLHS</code>) and the design point with the best prediction of the infill criterion is determined. Then, the parameter space is shrunk around the best design point. This process is repeated <code>opt.focussearch.maxit</code> (default 5) times and the best seen value of the infill criterion is passed back.</li>
<li><code>cmaes</code>: The optimal point is found with a covariance matrix adapting evolutionary strategy from the <strong>cmaes</strong> package. If the run fails, a random point is generated and a warning is given. Further control arguments can be provided in <code>opt.cmaes.control</code> as a list. </li>
<li><code>ea</code>: Use an evolutionary multiobjective optimization algorithm from the package <strong>emoa</strong> to determine the best point. The population size mu can be set by  <code>opt.ea.mu</code> (default value is 10). (mu+1) means that in each population only one child is generated using crossover und mutation operators. The parameters <code>eta</code> and <code>p</code> of the latter two operators can be adjusted via the attributes <code>opt.ea.sbx.eta</code>, <code>opt.ea.sbx.p</code>,<code>opt.ea.pm.eta</code> and <code>opt.ea.pm.p</code>. The default number of EA iterations is 500 and can be changed by <code>opt.ea.maxit</code> attribute.</li>
<li><code>nsga2</code>: Use the non-dominated sorting genetic algorithm from the package <strong>nsga2R</strong> to determine the best point. This algorithm should be used for <a href="multipoint.md">multi object optimization</a>.</li>
</ul>
<p>As all four infill optimization strategies do not guarantee to find the global optimum, users can set the number of restarts by the <code>opt.restarts</code> argument (default value is 1).
After conducting the desired number of restarts the design point with the best infill criterion value is passed back to the MBO loop.</p>
<p><em>Note:</em> Only the <code>focussearch</code> optimizer is suitable for for categorical parameters in the parameter set.</p>
<h2 id="setmbocontroltermination">setMBOControlTermination</h2>
<p>With this control function different criteria to stop the fitting process can be specified. You can set multiple different criteria and the first one that is met will terminate the fitting process.
You can set: </p>
<ul>
<li><code>iters</code>: The maximum number of iterations</li>
<li><code>time.budget</code>: A maximum running time in seconds with</li>
<li><code>target.fun.value</code>: A treshold for function evaluation (stop if a evaluation is better than a given value)</li>
<li><code>max.evals</code>: The maximum number of function evaluations</li>
</ul>
<p><em>Note:</em> You can also easily create your own stopping condition(s).</p>
<h2 id="setmbocontrolmultipoint">setMBOControlMultiPoint</h2>
<p>This extends a MBO control object with options for multipoint proposal. Multipoint proposal means, that multiple infill points are suggested and evaluated, which is especially useful in parallel batch evaluation. For a detailed introduction, check the <a href="multipoint.md">multi-point tutorial</a>.</p>
<h3 id="argument-method">Argument: <em>method</em></h3>
<p>Define the method used for multipoint proposals, currently 3 different methods are supported:</p>
<ul>
<li><code>cb</code>: Proposes multiple points by optimizing the confidence bound criterion <code>propose.points</code> times. Generally this works the same way as for the single point case, i.e. specify <code>infill.opt</code>. The  lambda parameters are drawn from an exp(1)-distribution.</li>
<li><code>multicrit</code>: Use a evolutionary multicriteria optimization. This is a (mu+1) type evolutionary algorithm and runs for <code>multicrit.maxit</code> generations. The population size is set to <code>propose.points</code>.</li>
<li><code>cl</code>: Proposes points by the constant liar strategy, which only makes sense if the confidence bound criterion is used as an infill criterion. In the first step the surrugate model is fitted based on the real data and the best point is calculated accordingly. Then, the function value of the best point is simply guessed by the worst seen function evaluation. This lie is used to update the model in order to propose subsequent point. The procedure is applied until the number of points is <code>propose.points</code>.</li>
</ul>
<h2 id="setmbocontrolmultifid">setMBOControlMultiFid</h2>
<p>Add multi-fidelity options to the <code>MBOControl</code> control object. This is useful when certain parameters increase the performance as well as the calculation cost. The idea is to combine the optimization of fast fitting low-fidelity models and more accurate but expensive high-fidelity models. The parameter on which the fidelity depends on is specified as <code>param</code> and the order of the values to train the learner with in <code>lvls</code>. The costs for the different levels can be specified or estimated by a model based on the execution time of the currently evaluated points.  </p>
<h2 id="setmbocontrolmulticrit">setMBOControlMultiCrit</h2>
<p>This adds multi-criteria optimization specific options to the control object. For details see the tutorial page on <a href="multicrit.md">multi-criteria optimization</a>.</p>
<p>The list of all attributes is provided in the software documentation.</p>
<h1 id="experiments-and-output">Experiments and Output</h1>
<p>Now we will apply the mbo() function to optimize the two objective functions.</p>
<pre><code class="r">control1 = makeMBOControl()
control1 = setMBOControlInfill(
  control = control1,
  crit = &quot;ei&quot;,
  opt = &quot;focussearch&quot;
)
control1 = setMBOControlTermination(
  control = control1,
  iters = 10
)

control2 = makeMBOControl()
control2 = setMBOControlInfill(
  control = control2,
  crit = &quot;mean&quot;,
  opt = &quot;focussearch&quot;
)
control2 = setMBOControlTermination(
  control = control2,
  iters = 10
)
</code></pre>

<h2 id="optimization-of-objfun1">Optimization of objfun1</h2>
<pre><code class="r">mbo1 = mbo(objfun1, design = design1, learner = surr.km, control = control1, 
           show.info = FALSE)
</code></pre>

<pre><code class="r">mbo1
#&gt; Recommended parameters:
#&gt; x=1.03,-11.1,-2.17,8.79,-0.205
#&gt; Objective: y = 15.517
#&gt; 
#&gt; Optimization path
#&gt; 25 + 10 entries in total, displaying last 10 (or less):
#&gt;              x1        x2         x3        x4          x5        y dob
#&gt; 26  -4.12605695 -11.54782 -5.6561831 10.981043  0.97068084 17.24931   1
#&gt; 27  -4.08285380 -12.35181 -7.4694699 30.601857  2.35503836 21.12274   2
#&gt; 28  -3.63459848 -11.16748 -0.2215530  9.728138 -0.59150240 16.78718   3
#&gt; 29   5.57099943 -10.94766  6.4746657  8.850323 -1.46103227 17.39163   4
#&gt; 30  -5.86468716 -10.71919 21.1616368  9.390592 -2.57240184 19.94265   5
#&gt; 31  17.32988712  -9.19084 -1.3564536  8.747120 -0.67130885 19.02134   6
#&gt; 32 -15.19605365 -11.42044 -0.7151403  8.628104 -4.72241544 19.05059   7
#&gt; 33  20.40116345 -11.35839  0.1671055  8.996293  0.07495621 19.37707   8
#&gt; 34  -0.09598261 -11.03762 -0.2741146  9.177053 -0.53188528 15.93757   9
#&gt; 35   1.02545547 -11.14438 -2.1749602  8.793208 -0.20463488 15.51709  10
#&gt;    eol error.message exec.time          ei error.model train.time
#&gt; 26  NA          &lt;NA&gt;     0.000 -0.13572183        &lt;NA&gt;      0.051
#&gt; 27  NA          &lt;NA&gt;     0.000 -0.04343611        &lt;NA&gt;      0.055
#&gt; 28  NA          &lt;NA&gt;     0.001 -0.06930553        &lt;NA&gt;      0.071
#&gt; 29  NA          &lt;NA&gt;     0.000 -0.07111303        &lt;NA&gt;      0.095
#&gt; 30  NA          &lt;NA&gt;     0.000 -0.04882831        &lt;NA&gt;      0.056
#&gt; 31  NA          &lt;NA&gt;     0.000 -0.05262250        &lt;NA&gt;      0.056
#&gt; 32  NA          &lt;NA&gt;     0.000 -0.04465761        &lt;NA&gt;      0.064
#&gt; 33  NA          &lt;NA&gt;     0.000 -0.07328187        &lt;NA&gt;      0.128
#&gt; 34  NA          &lt;NA&gt;     0.000 -0.15297708        &lt;NA&gt;      0.043
#&gt; 35  NA          &lt;NA&gt;     0.000 -0.09966843        &lt;NA&gt;      0.134
#&gt;    prop.type propose.time exec.timestamp        se     mean
#&gt; 26 infill_ei        1.311     1464944931 0.1701271 19.75186
#&gt; 27 infill_ei        1.378     1464944933 0.2315570 17.37290
#&gt; 28 infill_ei        1.213     1464944935 0.2334914 17.30166
#&gt; 29 infill_ei        1.396     1464944936 0.1798618 16.78846
#&gt; 30 infill_ei        1.373     1464944938 0.2711088 16.93882
#&gt; 31 infill_ei        1.391     1464944940 0.3026688 16.96312
#&gt; 32 infill_ei        1.418     1464944942 0.2836455 16.96928
#&gt; 33 infill_ei        1.432     1464944943 0.3412791 16.93965
#&gt; 34 infill_ei        1.487     1464944945 0.1966018 16.66667
#&gt; 35 infill_ei        1.395     1464944947 0.1522894 15.87113
</code></pre>

<p>The default output of mbo contains the best found parameter set and the optimzation path. The <code>MBOResult</code> object contains additional information, most importanty:</p>
<ul>
<li><code>x</code>: The best point of the parameter space</li>
<li><code>y</code>: The associated best value of the objective function</li>
<li><code>opt.path</code>: The optimization path. See <code>?ParamHelpers::OptPath</code> for further information.</li>
<li><code>models</code>: Depending on <code>store.model.at</code> in the <code>MBOControl</code> object, this contains zero, one or multiple surrogate models (default is to save the model generated after the last iteration).</li>
<li>...</li>
</ul>
<p>We can also change some arguments of the <code>MBOControl</code> object and run <code>mbo()</code> again:</p>
<pre><code class="r">control1$infill.crit = &quot;cb&quot;
control1$iters = 5L
mbo1 = mbo(objfun1, design = design1, learner = surr.km, control = control1, show.info = FALSE)
</code></pre>

<pre><code class="r">mbo1
#&gt; Recommended parameters:
#&gt; x=0.555,-6.83,6.51,4.35,-1.98
#&gt; Objective: y = 14.150
#&gt; 
#&gt; Optimization path
#&gt; 25 + 10 entries in total, displaying last 10 (or less):
#&gt;            x1         x2        x3         x4         x5        y dob eol
#&gt; 26 -4.6202804 -10.307607 16.870109   5.583554  -5.838394 19.02196   1  NA
#&gt; 27 -4.6471192 -13.116119 27.086733  17.835397 -10.658748 20.76526   2  NA
#&gt; 28 -7.2288110 -10.883809 23.025155   4.371716  -5.586849 19.92917   3  NA
#&gt; 29 -4.3568261 -15.236885 26.828895   5.941049  -6.683645 20.52021   4  NA
#&gt; 30  4.0754507 -10.199311 13.723630   4.539547 -11.181502 18.64589   5  NA
#&gt; 31 22.2615423  -8.420510 12.101165   4.913087 -17.755816 20.45866   6  NA
#&gt; 32  6.5055626 -10.197154 10.433297  14.087065 -22.152559 20.46904   7  NA
#&gt; 33  0.1582629  -7.663526 12.009755   4.105232  -3.323199 16.26470   8  NA
#&gt; 34  0.1181351  -7.649562 11.368898 -11.688711  -2.412261 18.07821   9  NA
#&gt; 35  0.5549428  -6.827755  6.507281   4.347422  -1.982138 14.14984  10  NA
#&gt;    error.message exec.time       cb error.model train.time prop.type
#&gt; 26          &lt;NA&gt;         0 19.65947        &lt;NA&gt;      0.047 infill_cb
#&gt; 27          &lt;NA&gt;         0 18.90914        &lt;NA&gt;      0.059 infill_cb
#&gt; 28          &lt;NA&gt;         0 18.92139        &lt;NA&gt;      0.041 infill_cb
#&gt; 29          &lt;NA&gt;         0 18.93866        &lt;NA&gt;      0.083 infill_cb
#&gt; 30          &lt;NA&gt;         0 18.82714        &lt;NA&gt;      0.041 infill_cb
#&gt; 31          &lt;NA&gt;         0 18.48476        &lt;NA&gt;      0.065 infill_cb
#&gt; 32          &lt;NA&gt;         0 18.50289        &lt;NA&gt;      0.081 infill_cb
#&gt; 33          &lt;NA&gt;         0 18.42454        &lt;NA&gt;      0.060 infill_cb
#&gt; 34          &lt;NA&gt;         0 16.11724        &lt;NA&gt;      0.064 infill_cb
#&gt; 35          &lt;NA&gt;         0 16.10702        &lt;NA&gt;      0.105 infill_cb
#&gt;    propose.time exec.timestamp        se     mean lambda
#&gt; 26        1.304     1464944949 0.1837239 19.84319      1
#&gt; 27        1.232     1464944951 0.1712301 19.08037      1
#&gt; 28        1.401     1464944952 0.1575612 19.07895      1
#&gt; 29        1.281     1464944954 0.1379985 19.07666      1
#&gt; 30        1.342     1464944956 0.1967692 19.02391      1
#&gt; 31        1.415     1464944957 0.1810977 18.66586      1
#&gt; 32        1.446     1464944959 0.1946998 18.69759      1
#&gt; 33        1.398     1464944961 0.2214879 18.64603      1
#&gt; 34        1.384     1464944963 0.2650294 16.38227      1
#&gt; 35        1.399     1464944964 0.2213947 16.32841      1
</code></pre>

<h2 id="optimization-of-objfun2">Optimization of objfun2</h2>
<p>Now let us use <strong>mlrMBO</strong> to optimize <code>objfun2</code>, which contains one categorical variable.
As we have already mentioned before, in case of factor variables only <code>focussearch</code> is suitable and kriging cannot be used as a surrogate model.
If we would use <code>mean</code> as the infill criterion, any kind of model which can handle factors variables is possible (like regression trees, random forests, linear models and many others).</p>
<pre><code class="r">mbo2 = mbo(objfun2, design = design2, learner = surr.rf, control = control2, show.info = FALSE)
</code></pre>

<pre><code class="r">mbo2
#&gt; Recommended parameters:
#&gt; j=0.462; k=2; method=b
#&gt; Objective: y = 2.236
#&gt; 
#&gt; Optimization path
#&gt; 15 + 10 entries in total, displaying last 10 (or less):
#&gt;            j k method        y dob eol error.message exec.time      mean
#&gt; 16 0.4757427 2      b 2.235904   1  NA          &lt;NA&gt;         0 -2.026581
#&gt; 17 0.4849609 2      b 2.235560   2  NA          &lt;NA&gt;         0 -2.082962
#&gt; 18 0.4717123 2      b 2.235995   3  NA          &lt;NA&gt;         0 -2.119596
#&gt; 19 0.4777613 2      b 2.235845   4  NA          &lt;NA&gt;         0 -2.138953
#&gt; 20 0.4674350 2      b 2.236052   5  NA          &lt;NA&gt;         0 -2.152694
#&gt; 21 0.4660672 2      b 2.236061   6  NA          &lt;NA&gt;         0 -2.161759
#&gt; 22 0.4622545 2      b 2.236066   7  NA          &lt;NA&gt;         0 -2.172939
#&gt; 23 0.4673248 2      b 2.236053   8  NA          &lt;NA&gt;         0 -2.174135
#&gt; 24 0.4533910 2      b 2.235950   9  NA          &lt;NA&gt;         0 -2.175038
#&gt; 25 0.4587362 2      b 2.236041  10  NA          &lt;NA&gt;         0 -2.187947
#&gt;    error.model train.time   prop.type propose.time exec.timestamp
#&gt; 16        &lt;NA&gt;      0.008 infill_mean        0.517     1464944965
#&gt; 17        &lt;NA&gt;      0.006 infill_mean        0.524     1464944966
#&gt; 18        &lt;NA&gt;      0.007 infill_mean        0.515     1464944967
#&gt; 19        &lt;NA&gt;      0.007 infill_mean        0.534     1464944968
#&gt; 20        &lt;NA&gt;      0.006 infill_mean        0.515     1464944969
#&gt; 21        &lt;NA&gt;      0.007 infill_mean        0.521     1464944969
#&gt; 22        &lt;NA&gt;      0.007 infill_mean        0.521     1464944970
#&gt; 23        &lt;NA&gt;      0.008 infill_mean        0.522     1464944971
#&gt; 24        &lt;NA&gt;      0.008 infill_mean        0.522     1464944972
#&gt; 25        &lt;NA&gt;      0.008 infill_mean        0.541     1464944973
</code></pre>

<p>If we want to use the expected improvement <code>ei</code> or (lower) confidence bound <code>cb</code> infill criteria, the <code>predict.type</code> attribute of the learner has be set to <code>se</code>. A list of regression learners which support it can be viewed by:</p>
<pre><code class="r">listLearners(obj = &quot;regr&quot;, properties = &quot;se&quot;)
</code></pre>

<p>We modify the random forest to predict the standard error and optimize <code>objfun2</code> by the <code>ei</code> infill criterion.</p>
<pre><code class="r">learner_rf = makeLearner(&quot;regr.randomForest&quot;, predict.type = &quot;se&quot;)
control2$infill.crit = &quot;ei&quot;
mbo2 = mbo(objfun2, design = design2, learner = learner_rf, 
           control = control2, show.info = FALSE)
</code></pre>

<pre><code class="r">mbo2
#&gt; Recommended parameters:
#&gt; j=0.428; k=2; method=b
#&gt; Objective: y = 2.235
#&gt; 
#&gt; Optimization path
#&gt; 15 + 10 entries in total, displaying last 10 (or less):
#&gt;            j k method        y dob eol error.message exec.time
#&gt; 16 0.4087665 2      b 2.232701   1  NA          &lt;NA&gt;         0
#&gt; 17 0.4087998 2      b 2.232705   2  NA          &lt;NA&gt;         0
#&gt; 18 0.4033413 2      b 2.232003   3  NA          &lt;NA&gt;         0
#&gt; 19 0.3865188 2      b 2.229420   4  NA          &lt;NA&gt;         0
#&gt; 20 0.3744102 2      b 2.227171   5  NA          &lt;NA&gt;         0
#&gt; 21 0.3983756 2      b 2.231306   6  NA          &lt;NA&gt;         0
#&gt; 22 0.3475329 2      b 2.221011   7  NA          &lt;NA&gt;         0
#&gt; 23 0.3283410 2      b 2.215630   8  NA          &lt;NA&gt;         0
#&gt; 24 0.3200959 2      b 2.213068   9  NA          &lt;NA&gt;         0
#&gt; 25 0.3247575 2      b 2.214535  10  NA          &lt;NA&gt;         0
#&gt;              ei error.model train.time prop.type propose.time
#&gt; 16 -0.001032499        &lt;NA&gt;      0.174 infill_ei       34.013
#&gt; 17 -0.003238726        &lt;NA&gt;      0.190 infill_ei       31.291
#&gt; 18 -0.002051443        &lt;NA&gt;      0.173 infill_ei       31.707
#&gt; 19 -0.002915200        &lt;NA&gt;      0.217 infill_ei       31.168
#&gt; 20 -0.002450279        &lt;NA&gt;      0.207 infill_ei       31.175
#&gt; 21 -0.002619635        &lt;NA&gt;      0.175 infill_ei       31.820
#&gt; 22 -0.004576781        &lt;NA&gt;      0.210 infill_ei       32.282
#&gt; 23 -0.005052992        &lt;NA&gt;      0.187 infill_ei       31.942
#&gt; 24 -0.003741054        &lt;NA&gt;      0.181 infill_ei       31.752
#&gt; 25 -0.004406909        &lt;NA&gt;      0.185 infill_ei       32.442
#&gt;    exec.timestamp         se     mean
#&gt; 16     1464945008 0.14104875 1.944803
#&gt; 17     1464945040 0.14306666 2.004142
#&gt; 18     1464945072 0.10511671 2.058761
#&gt; 19     1464945104 0.13876233 2.006720
#&gt; 20     1464945136 0.09833344 2.080229
#&gt; 21     1464945168 0.08121638 2.116259
#&gt; 22     1464945201 0.10829462 2.090093
#&gt; 23     1464945234 0.12091591 2.072615
#&gt; 24     1464945266 0.10710490 2.082343
#&gt; 25     1464945299 0.08455944 2.130153
</code></pre>

<p>Finally, if a learner, which does not support the <code>se</code> prediction type, should be applied for the optimization with the <code>ei</code> infill criterion, it is  possible to create a bagging model with the desired characteristics. For details on how to do it take a look at the <a href="https://mlr-org.github.io/mlr-tutorial/devel/html/bagging/index.html">bagging section</a> in the <code>mlr</code> tutorial.</p></div>
            
        </div>

        <footer class="col-md-12">
            <hr>
            
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>

        <script src="../js/jquery-1.10.2.min.js"></script>
        <script src="../js/bootstrap-3.0.3.min.js"></script>
        <script src="../js/highlight.pack.js"></script>
        <script>var base_url = '..';</script>
        <script data-main="../mkdocs/js/search.js" src="../mkdocs/js/require.js"></script>
        <script src="../js/base.js"></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            From here you can search these documents. Enter
                            your search terms below.
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>

    </body>
</html>
