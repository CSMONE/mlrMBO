{
    "docs": [
        {
            "location": "/index.html", 
            "text": "mlrMBO Tutorial\n\n\nmlrMBO\n is a framework for the (sequential) Model Based parameter Optimization.\nThe goal is to optimize numeric or discrete influence parameters\nof a non-linear black box function like an industrial simulator or  a time-consuming algorithm.\n\n\nIn the following we provide an in-depth introduction to \nmlrMBO\n. An introductory example serves as a quickstart guide.\nNote that our focus is on your comprehension of the basic functions and\napplications. For detailed technical information and manual pages, please refer to\nthe package's \nmanual pages\n. They are regularly updated and reflect the documentation of the current packages on CRAN.\n\n\n\n\nQuickstart\n\n\nIn Depth Introduction\n\n\nFurther advanced topics:\n\n\nParallelization\n Make use of multicore CPUs and other distributed computing methods.", 
            "title": "Home"
        }, 
        {
            "location": "/index.html#mlrmbo-tutorial", 
            "text": "mlrMBO  is a framework for the (sequential) Model Based parameter Optimization.\nThe goal is to optimize numeric or discrete influence parameters\nof a non-linear black box function like an industrial simulator or  a time-consuming algorithm.  In the following we provide an in-depth introduction to  mlrMBO . An introductory example serves as a quickstart guide.\nNote that our focus is on your comprehension of the basic functions and\napplications. For detailed technical information and manual pages, please refer to\nthe package's  manual pages . They are regularly updated and reflect the documentation of the current packages on CRAN.   Quickstart  In Depth Introduction  Further advanced topics:  Parallelization  Make use of multicore CPUs and other distributed computing methods.", 
            "title": "mlrMBO Tutorial"
        }, 
        {
            "location": "/quickstart/index.html", 
            "text": "Introductory example\n\n\nInfo:\n this guide gives you an overview of the typical optimization workflow with \nmrMBO\n. For a much more\ndetailed introduction see \nthe next chapter\n.\n\n\nHere we provide a quickstart example for you to make yourself familiar with \nmlrMBO\n. We aim to optimize the one dimensional Rastrigin function using model-based optimization. Instead of writing this function by hand, we make use of the smoof package, which offers a lot of common single objective optimization functions.\n\n\nlibrary(smoof)\nlibrary(mlr)\nlibrary(mlrMBO)\nlibrary(ParamHelpers)\nobj.fun = makeRastriginFunction(1)\n\n\n\n\nNote:\n Since all this stuff here is still under developement it might be neccessary to install the github developement version of the ParamHelpers package via the \ndevtools::install_github\n function.\n\n\nWe decide to use kriging as our surrogate model and to do 10 sequential optimization steps. Furthermore we use Expected Improvement (EI) as the infill criterion, i. e., the criterion which determines which point(s) of the objective function should be evaluated in each iterations (keep in mind, that using EI as the infill criterion needs the learner to support standard error estimation). \n\n\nAs a last step we have to generate an initial design on which we evaluate our model in the beginning. We use \nParamHelpers::generateDesign\n to generate 10 points in a latin hypercube design.\n\n\nlearner = makeLearner(\nregr.km\n, predict.type = \nse\n, covtype = \nmatern3_2\n)\ncontrol = makeMBOControl()\ncontrol = setMBOControlTermination(control, iters = 10)\ncontrol = setMBOControlInfill(control, crit = \nei\n)\ndesign = generateDesign(n = 10, par.set = smoof::getParamSet(obj.fun))\n\n\n\n\nFinally we start the optimization process and print the result object.\n\n\nresult = mbo(obj.fun, design = design, learner = learner, control = control, \n             show.info = TRUE)\n#\n Computing y column(s) for design. Not provided.\n#\n [mbo] 0: x=4.17 : y = 22.8 : 0.0 secs : initdesign\n#\n [mbo] 0: x=2.74 : y = 18.4 : 0.0 secs : initdesign\n#\n [mbo] 0: x=0.272 : y = 11.5 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-1.65 : y = 18.4 : 0.0 secs : initdesign\n#\n [mbo] 0: x=2.03 : y = 4.32 : 0.0 secs : initdesign\n#\n [mbo] 0: x=3.11 : y = 12.1 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-2.44 : y = 25.3 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-3.89 : y = 17.4 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-0.769 : y = 9.37 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-4.37 : y = 35.9 : 0.0 secs : initdesign\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -42.16228 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       42.162  |proj g|=       1.9768\n#\n At iterate     1  f =       35.839  |proj g|=       0.31573\n#\n At iterate     2  f =        35.68  |proj g|=             0\n#\n \n#\n iterations 2\n#\n function evaluations 3\n#\n segments explored during Cauchy searches 2\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 35.6801\n#\n \n#\n F = 35.6801\n#\n final  value 35.680121 \n#\n converged\n#\n [mbo] 1: x=-2.05 : y = 4.7 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -50.85536 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       50.855  |proj g|=       1.8606\n#\n At iterate     1  f =       39.744  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 39.7438\n#\n \n#\n F = 39.7438\n#\n final  value 39.743841 \n#\n converged\n#\n [mbo] 2: x=3.62 : y = 30.3 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -46.05445 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       46.054  |proj g|=      0.45522\n#\n At iterate     1  f =       43.926  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 43.9265\n#\n \n#\n F = 43.9265\n#\n final  value 43.926485 \n#\n converged\n#\n [mbo] 3: x=-3.55 : y = 32.2 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -49.18026 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=        49.18  |proj g|=      0.30591\n#\n At iterate     1  f =       48.175  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 48.1752\n#\n \n#\n F = 48.1752\n#\n final  value 48.175161 \n#\n converged\n#\n [mbo] 4: x=3.72 : y = 25.6 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -52.05361 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       52.054  |proj g|=      0.30731\n#\n At iterate     1  f =       51.605  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 51.6047\n#\n \n#\n F = 51.6047\n#\n final  value 51.604695 \n#\n converged\n#\n [mbo] 5: x=-3.79 : y = 22 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -54.14426 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       54.144  |proj g|=       5.6298\n#\n At iterate     1  f =       54.011  |proj g|=       0.22682\n#\n At iterate     2  f =       53.963  |proj g|=        0.1198\n#\n At iterate     3  f =       53.963  |proj g|=     0.0074116\n#\n At iterate     4  f =       53.963  |proj g|=     1.392e-05\n#\n At iterate     5  f =       53.963  |proj g|=    1.5912e-09\n#\n \n#\n iterations 5\n#\n function evaluations 8\n#\n segments explored during Cauchy searches 5\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 1.59118e-09\n#\n final function value 53.9628\n#\n \n#\n F = 53.9628\n#\n final  value 53.962788 \n#\n converged\n#\n [mbo] 6: x=1.94 : y = 4.5 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -57.42694 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       57.427  |proj g|=      0.37626\n#\n At iterate     1  f =       56.944  |proj g|=       0.31386\n#\n At iterate     2  f =       56.668  |proj g|=       0.24689\n#\n At iterate     3  f =       56.657  |proj g|=       0.52416\n#\n At iterate     4  f =       56.656  |proj g|=      0.025438\n#\n At iterate     5  f =       56.656  |proj g|=    0.00038393\n#\n At iterate     6  f =       56.656  |proj g|=    2.9003e-07\n#\n \n#\n iterations 6\n#\n function evaluations 9\n#\n segments explored during Cauchy searches 6\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 2.90031e-07\n#\n final function value 56.6555\n#\n \n#\n F = 56.6555\n#\n final  value 56.655547 \n#\n converged\n#\n [mbo] 7: x=-1.96 : y = 4.13 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -68.84067 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       68.841  |proj g|=       1.4873\n#\n At iterate     1  f =       63.315  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 63.3155\n#\n \n#\n F = 63.3155\n#\n final  value 63.315486 \n#\n converged\n#\n [mbo] 8: x=1.13 : y = 4.36 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -64.75254 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       64.753  |proj g|=      0.52075\n#\n At iterate     1  f =       63.962  |proj g|=       0.43422\n#\n At iterate     2  f =       63.285  |proj g|=       0.30913\n#\n At iterate     3  f =       63.269  |proj g|=        1.1478\n#\n At iterate     4  f =       63.261  |proj g|=      0.074181\n#\n At iterate     5  f =       63.261  |proj g|=     0.0026487\n#\n At iterate     6  f =       63.261  |proj g|=    6.4719e-06\n#\n \n#\n iterations 6\n#\n function evaluations 9\n#\n segments explored during Cauchy searches 6\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 6.47193e-06\n#\n final function value 63.2613\n#\n \n#\n F = 63.2613\n#\n final  value 63.261339 \n#\n converged\n#\n [mbo] 9: x=1.32 : y = 16 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -76.513 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       76.513  |proj g|=       1.3051\n#\n At iterate     1  f =       70.586  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 70.586\n#\n \n#\n F = 70.586\n#\n final  value 70.586034 \n#\n converged\n#\n [mbo] 10: x=1.41 : y = 20.4 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -85.50927 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       85.509  |proj g|=       1.4973\n#\n At iterate     1  f =       82.144  |proj g|=        2.0383\n#\n ys=-2.885e+00  -gs= 2.242e+00, BFGS update SKIPPED\n#\n At iterate     2  f =       73.855  |proj g|=             0\n#\n \n#\n iterations 2\n#\n function evaluations 3\n#\n segments explored during Cauchy searches 2\n#\n BFGS updates skipped 1\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 73.855\n#\n \n#\n F = 73.855\n#\n final  value 73.854960 \n#\n converged\nprint(result)\n#\n Recommended parameters:\n#\n x=-1.96\n#\n Objective: y = 4.134\n#\n \n#\n Optimization path\n#\n 10 + 10 entries in total, displaying last 10 (or less):\n#\n            x         y dob eol error.message exec.time         ei\n#\n 11 -2.050284  4.698625   1  NA          \nNA\n     0.000 -0.2288446\n#\n 12  3.623820 30.255372   2  NA          \nNA\n     0.000 -0.3726289\n#\n 13 -3.546339 32.155650   3  NA          \nNA\n     0.001 -0.3422541\n#\n 14  3.722633 25.569052   4  NA          \nNA\n     0.000 -0.3188126\n#\n 15 -3.787732 21.998262   5  NA          \nNA\n     0.000 -0.2604270\n#\n 16  1.938323  4.498631   6  NA          \nNA\n     0.000 -1.1183878\n#\n 17 -1.961884  4.134397   7  NA          \nNA\n     0.000 -0.9190494\n#\n 18  1.128445  4.357019   8  NA          \nNA\n     0.000 -0.4194018\n#\n 19  1.319269 15.956642   9  NA          \nNA\n     0.000 -1.4304011\n#\n 20  1.409904 20.427879  10  NA          \nNA\n     0.000 -0.4781482\n#\n    error.model train.time prop.type propose.time exec.timestamp        se\n#\n 11        \nNA\n      0.020 infill_ei        0.222     1464952858  8.576909\n#\n 12        \nNA\n      0.019 infill_ei        0.222     1464952859  8.972263\n#\n 13        \nNA\n      0.019 infill_ei        0.225     1464952859  9.408390\n#\n 14        \nNA\n      0.018 infill_ei        0.252     1464952860  9.843812\n#\n 15        \nNA\n      0.017 infill_ei        0.234     1464952860  9.651479\n#\n 16        \nNA\n      0.020 infill_ei        0.256     1464952861  5.725409\n#\n 17        \nNA\n      0.021 infill_ei        0.261     1464952862  4.622039\n#\n 18        \nNA\n      0.017 infill_ei        0.256     1464952862 10.029210\n#\n 19        \nNA\n      0.022 infill_ei        0.281     1464952863  7.264680\n#\n 20        \nNA\n      0.019 infill_ei        0.267     1464952863  9.935517\n#\n         mean\n#\n 11 17.538842\n#\n 12 16.371550\n#\n 13 17.528535\n#\n 14 18.653697\n#\n 15 19.147651\n#\n 16  7.231362\n#\n 17  6.618227\n#\n 18 17.570495\n#\n 19  7.787929\n#\n 20 16.790109\n\n\n\n\nExample run\n\n\nThere is also the function \nexampleRun\n, which is useful to figure out how \nmbo\n works and to visualize the results.\n\n\nex = exampleRun(obj.fun, control = control, show.info = FALSE)\n#\n Warning in makeMboLearner(control, fun): filter.proposed.points is not set\n#\n in the control object. This might lead to the 'leading minor of order ...'\n#\n error during model fit.\n\n\n\n\nprint(ex)\n#\n MBOExampleRun\n#\n Number of parameters        : 1\n#\n Parameter names             : x\n#\n Parameter types             : numericvector\n#\n Global Opt (known)          : 0.0000e+00\n#\n Gap for best point          : 9.1272e-04\n#\n True points per dim.        : 50\n#\n Objectives                    : 1\n#\n Points proposed per iter      : 1\n#\n \n#\n Infill criterion              : ei\n#\n Infill optimizer              : focussearch\n#\n Infill optimizer restarts     : 1\n#\n Final point by                : best.true.y\n#\n Learner                     : regr.km\n#\n Learner settings:\n#\n jitter=FALSE,nugget.stability=1e-08\n#\n Recommended parameters:\n#\n x=-0.00214\n#\n Objective: y = 9.127e-04\nplotExampleRun(ex)\n\n\n\n\n\n\nOr alternatively for a two dimensional function:\n\n\nobj.fun2 = makeRastriginFunction(2L)\ndesign2 = generateDesign(n = 10, par.set = smoof::getParamSet(obj.fun2))\nex2 = exampleRun(obj.fun2, control = control, show.info = FALSE)\n#\n Warning in makeMboLearner(control, fun): filter.proposed.points is not set\n#\n in the control object. This might lead to the 'leading minor of order ...'\n#\n error during model fit.\n\n\n\n\nprint(ex2)\n#\n MBOExampleRun\n#\n Number of parameters        : 2\n#\n Parameter names             : x1,x2\n#\n Parameter types             : numericvector\n#\n Global Opt (known)          : 0.0000e+00\n#\n Gap for best point          : 2.5901e+00\n#\n True points per dim.        : 50\n#\n Objectives                    : 1\n#\n Points proposed per iter      : 1\n#\n \n#\n Infill criterion              : ei\n#\n Infill optimizer              : focussearch\n#\n Infill optimizer restarts     : 1\n#\n Final point by                : best.true.y\n#\n Learner                     : regr.km\n#\n Learner settings:\n#\n jitter=FALSE,nugget.stability=1e-08\n#\n Recommended parameters:\n#\n x=0.0525,1.07\n#\n Objective: y = 2.590e+00\nplotExampleRun(ex2)\n#\n Loading required package: gridExtra\n\n\n\n\n\n\n#\n Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#\n sort(unique(data$y)), : all z values are equal\n#\n Warning: Not possible to generate contour data\n#\n Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#\n sort(unique(data$y)), : all z values are equal\n#\n Warning: Not possible to generate contour data\n\n\n\n\n\n\n#\n Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#\n sort(unique(data$y)), : all z values are equal\n\n#\n Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#\n sort(unique(data$y)), : Not possible to generate contour data\n#\n Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#\n sort(unique(data$y)), : all z values are equal\n#\n Warning: Not possible to generate contour data\n\n\n\n\n\n\n#\n Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#\n sort(unique(data$y)), : all z values are equal\n\n#\n Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#\n sort(unique(data$y)), : Not possible to generate contour data\n#\n Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#\n sort(unique(data$y)), : all z values are equal\n#\n Warning: Not possible to generate contour data\n\n\n\n\n\n\n#\n Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#\n sort(unique(data$y)), : all z values are equal\n\n#\n Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#\n sort(unique(data$y)), : Not possible to generate contour data\n#\n Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#\n sort(unique(data$y)), : all z values are equal\n#\n Warning: Not possible to generate contour data", 
            "title": "Quickstart"
        }, 
        {
            "location": "/quickstart/index.html#introductory-example", 
            "text": "Info:  this guide gives you an overview of the typical optimization workflow with  mrMBO . For a much more\ndetailed introduction see  the next chapter .  Here we provide a quickstart example for you to make yourself familiar with  mlrMBO . We aim to optimize the one dimensional Rastrigin function using model-based optimization. Instead of writing this function by hand, we make use of the smoof package, which offers a lot of common single objective optimization functions.  library(smoof)\nlibrary(mlr)\nlibrary(mlrMBO)\nlibrary(ParamHelpers)\nobj.fun = makeRastriginFunction(1)  Note:  Since all this stuff here is still under developement it might be neccessary to install the github developement version of the ParamHelpers package via the  devtools::install_github  function.  We decide to use kriging as our surrogate model and to do 10 sequential optimization steps. Furthermore we use Expected Improvement (EI) as the infill criterion, i. e., the criterion which determines which point(s) of the objective function should be evaluated in each iterations (keep in mind, that using EI as the infill criterion needs the learner to support standard error estimation).   As a last step we have to generate an initial design on which we evaluate our model in the beginning. We use  ParamHelpers::generateDesign  to generate 10 points in a latin hypercube design.  learner = makeLearner( regr.km , predict.type =  se , covtype =  matern3_2 )\ncontrol = makeMBOControl()\ncontrol = setMBOControlTermination(control, iters = 10)\ncontrol = setMBOControlInfill(control, crit =  ei )\ndesign = generateDesign(n = 10, par.set = smoof::getParamSet(obj.fun))  Finally we start the optimization process and print the result object.  result = mbo(obj.fun, design = design, learner = learner, control = control, \n             show.info = TRUE)\n#  Computing y column(s) for design. Not provided.\n#  [mbo] 0: x=4.17 : y = 22.8 : 0.0 secs : initdesign\n#  [mbo] 0: x=2.74 : y = 18.4 : 0.0 secs : initdesign\n#  [mbo] 0: x=0.272 : y = 11.5 : 0.0 secs : initdesign\n#  [mbo] 0: x=-1.65 : y = 18.4 : 0.0 secs : initdesign\n#  [mbo] 0: x=2.03 : y = 4.32 : 0.0 secs : initdesign\n#  [mbo] 0: x=3.11 : y = 12.1 : 0.0 secs : initdesign\n#  [mbo] 0: x=-2.44 : y = 25.3 : 0.0 secs : initdesign\n#  [mbo] 0: x=-3.89 : y = 17.4 : 0.0 secs : initdesign\n#  [mbo] 0: x=-0.769 : y = 9.37 : 0.0 secs : initdesign\n#  [mbo] 0: x=-4.37 : y = 35.9 : 0.0 secs : initdesign\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -42.16228 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       42.162  |proj g|=       1.9768\n#  At iterate     1  f =       35.839  |proj g|=       0.31573\n#  At iterate     2  f =        35.68  |proj g|=             0\n#  \n#  iterations 2\n#  function evaluations 3\n#  segments explored during Cauchy searches 2\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 35.6801\n#  \n#  F = 35.6801\n#  final  value 35.680121 \n#  converged\n#  [mbo] 1: x=-2.05 : y = 4.7 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -50.85536 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       50.855  |proj g|=       1.8606\n#  At iterate     1  f =       39.744  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 39.7438\n#  \n#  F = 39.7438\n#  final  value 39.743841 \n#  converged\n#  [mbo] 2: x=3.62 : y = 30.3 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -46.05445 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       46.054  |proj g|=      0.45522\n#  At iterate     1  f =       43.926  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 43.9265\n#  \n#  F = 43.9265\n#  final  value 43.926485 \n#  converged\n#  [mbo] 3: x=-3.55 : y = 32.2 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -49.18026 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=        49.18  |proj g|=      0.30591\n#  At iterate     1  f =       48.175  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 48.1752\n#  \n#  F = 48.1752\n#  final  value 48.175161 \n#  converged\n#  [mbo] 4: x=3.72 : y = 25.6 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -52.05361 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       52.054  |proj g|=      0.30731\n#  At iterate     1  f =       51.605  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 51.6047\n#  \n#  F = 51.6047\n#  final  value 51.604695 \n#  converged\n#  [mbo] 5: x=-3.79 : y = 22 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -54.14426 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       54.144  |proj g|=       5.6298\n#  At iterate     1  f =       54.011  |proj g|=       0.22682\n#  At iterate     2  f =       53.963  |proj g|=        0.1198\n#  At iterate     3  f =       53.963  |proj g|=     0.0074116\n#  At iterate     4  f =       53.963  |proj g|=     1.392e-05\n#  At iterate     5  f =       53.963  |proj g|=    1.5912e-09\n#  \n#  iterations 5\n#  function evaluations 8\n#  segments explored during Cauchy searches 5\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 0\n#  norm of the final projected gradient 1.59118e-09\n#  final function value 53.9628\n#  \n#  F = 53.9628\n#  final  value 53.962788 \n#  converged\n#  [mbo] 6: x=1.94 : y = 4.5 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -57.42694 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       57.427  |proj g|=      0.37626\n#  At iterate     1  f =       56.944  |proj g|=       0.31386\n#  At iterate     2  f =       56.668  |proj g|=       0.24689\n#  At iterate     3  f =       56.657  |proj g|=       0.52416\n#  At iterate     4  f =       56.656  |proj g|=      0.025438\n#  At iterate     5  f =       56.656  |proj g|=    0.00038393\n#  At iterate     6  f =       56.656  |proj g|=    2.9003e-07\n#  \n#  iterations 6\n#  function evaluations 9\n#  segments explored during Cauchy searches 6\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 0\n#  norm of the final projected gradient 2.90031e-07\n#  final function value 56.6555\n#  \n#  F = 56.6555\n#  final  value 56.655547 \n#  converged\n#  [mbo] 7: x=-1.96 : y = 4.13 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -68.84067 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       68.841  |proj g|=       1.4873\n#  At iterate     1  f =       63.315  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 63.3155\n#  \n#  F = 63.3155\n#  final  value 63.315486 \n#  converged\n#  [mbo] 8: x=1.13 : y = 4.36 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -64.75254 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       64.753  |proj g|=      0.52075\n#  At iterate     1  f =       63.962  |proj g|=       0.43422\n#  At iterate     2  f =       63.285  |proj g|=       0.30913\n#  At iterate     3  f =       63.269  |proj g|=        1.1478\n#  At iterate     4  f =       63.261  |proj g|=      0.074181\n#  At iterate     5  f =       63.261  |proj g|=     0.0026487\n#  At iterate     6  f =       63.261  |proj g|=    6.4719e-06\n#  \n#  iterations 6\n#  function evaluations 9\n#  segments explored during Cauchy searches 6\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 0\n#  norm of the final projected gradient 6.47193e-06\n#  final function value 63.2613\n#  \n#  F = 63.2613\n#  final  value 63.261339 \n#  converged\n#  [mbo] 9: x=1.32 : y = 16 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -76.513 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       76.513  |proj g|=       1.3051\n#  At iterate     1  f =       70.586  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 70.586\n#  \n#  F = 70.586\n#  final  value 70.586034 \n#  converged\n#  [mbo] 10: x=1.41 : y = 20.4 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -85.50927 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       85.509  |proj g|=       1.4973\n#  At iterate     1  f =       82.144  |proj g|=        2.0383\n#  ys=-2.885e+00  -gs= 2.242e+00, BFGS update SKIPPED\n#  At iterate     2  f =       73.855  |proj g|=             0\n#  \n#  iterations 2\n#  function evaluations 3\n#  segments explored during Cauchy searches 2\n#  BFGS updates skipped 1\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 73.855\n#  \n#  F = 73.855\n#  final  value 73.854960 \n#  converged\nprint(result)\n#  Recommended parameters:\n#  x=-1.96\n#  Objective: y = 4.134\n#  \n#  Optimization path\n#  10 + 10 entries in total, displaying last 10 (or less):\n#             x         y dob eol error.message exec.time         ei\n#  11 -2.050284  4.698625   1  NA           NA      0.000 -0.2288446\n#  12  3.623820 30.255372   2  NA           NA      0.000 -0.3726289\n#  13 -3.546339 32.155650   3  NA           NA      0.001 -0.3422541\n#  14  3.722633 25.569052   4  NA           NA      0.000 -0.3188126\n#  15 -3.787732 21.998262   5  NA           NA      0.000 -0.2604270\n#  16  1.938323  4.498631   6  NA           NA      0.000 -1.1183878\n#  17 -1.961884  4.134397   7  NA           NA      0.000 -0.9190494\n#  18  1.128445  4.357019   8  NA           NA      0.000 -0.4194018\n#  19  1.319269 15.956642   9  NA           NA      0.000 -1.4304011\n#  20  1.409904 20.427879  10  NA           NA      0.000 -0.4781482\n#     error.model train.time prop.type propose.time exec.timestamp        se\n#  11         NA       0.020 infill_ei        0.222     1464952858  8.576909\n#  12         NA       0.019 infill_ei        0.222     1464952859  8.972263\n#  13         NA       0.019 infill_ei        0.225     1464952859  9.408390\n#  14         NA       0.018 infill_ei        0.252     1464952860  9.843812\n#  15         NA       0.017 infill_ei        0.234     1464952860  9.651479\n#  16         NA       0.020 infill_ei        0.256     1464952861  5.725409\n#  17         NA       0.021 infill_ei        0.261     1464952862  4.622039\n#  18         NA       0.017 infill_ei        0.256     1464952862 10.029210\n#  19         NA       0.022 infill_ei        0.281     1464952863  7.264680\n#  20         NA       0.019 infill_ei        0.267     1464952863  9.935517\n#          mean\n#  11 17.538842\n#  12 16.371550\n#  13 17.528535\n#  14 18.653697\n#  15 19.147651\n#  16  7.231362\n#  17  6.618227\n#  18 17.570495\n#  19  7.787929\n#  20 16.790109", 
            "title": "Introductory example"
        }, 
        {
            "location": "/quickstart/index.html#example-run", 
            "text": "There is also the function  exampleRun , which is useful to figure out how  mbo  works and to visualize the results.  ex = exampleRun(obj.fun, control = control, show.info = FALSE)\n#  Warning in makeMboLearner(control, fun): filter.proposed.points is not set\n#  in the control object. This might lead to the 'leading minor of order ...'\n#  error during model fit.  print(ex)\n#  MBOExampleRun\n#  Number of parameters        : 1\n#  Parameter names             : x\n#  Parameter types             : numericvector\n#  Global Opt (known)          : 0.0000e+00\n#  Gap for best point          : 9.1272e-04\n#  True points per dim.        : 50\n#  Objectives                    : 1\n#  Points proposed per iter      : 1\n#  \n#  Infill criterion              : ei\n#  Infill optimizer              : focussearch\n#  Infill optimizer restarts     : 1\n#  Final point by                : best.true.y\n#  Learner                     : regr.km\n#  Learner settings:\n#  jitter=FALSE,nugget.stability=1e-08\n#  Recommended parameters:\n#  x=-0.00214\n#  Objective: y = 9.127e-04\nplotExampleRun(ex)   Or alternatively for a two dimensional function:  obj.fun2 = makeRastriginFunction(2L)\ndesign2 = generateDesign(n = 10, par.set = smoof::getParamSet(obj.fun2))\nex2 = exampleRun(obj.fun2, control = control, show.info = FALSE)\n#  Warning in makeMboLearner(control, fun): filter.proposed.points is not set\n#  in the control object. This might lead to the 'leading minor of order ...'\n#  error during model fit.  print(ex2)\n#  MBOExampleRun\n#  Number of parameters        : 2\n#  Parameter names             : x1,x2\n#  Parameter types             : numericvector\n#  Global Opt (known)          : 0.0000e+00\n#  Gap for best point          : 2.5901e+00\n#  True points per dim.        : 50\n#  Objectives                    : 1\n#  Points proposed per iter      : 1\n#  \n#  Infill criterion              : ei\n#  Infill optimizer              : focussearch\n#  Infill optimizer restarts     : 1\n#  Final point by                : best.true.y\n#  Learner                     : regr.km\n#  Learner settings:\n#  jitter=FALSE,nugget.stability=1e-08\n#  Recommended parameters:\n#  x=0.0525,1.07\n#  Objective: y = 2.590e+00\nplotExampleRun(ex2)\n#  Loading required package: gridExtra   #  Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#  sort(unique(data$y)), : all z values are equal\n#  Warning: Not possible to generate contour data\n#  Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#  sort(unique(data$y)), : all z values are equal\n#  Warning: Not possible to generate contour data   #  Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#  sort(unique(data$y)), : all z values are equal\n\n#  Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#  sort(unique(data$y)), : Not possible to generate contour data\n#  Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#  sort(unique(data$y)), : all z values are equal\n#  Warning: Not possible to generate contour data   #  Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#  sort(unique(data$y)), : all z values are equal\n\n#  Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#  sort(unique(data$y)), : Not possible to generate contour data\n#  Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#  sort(unique(data$y)), : all z values are equal\n#  Warning: Not possible to generate contour data   #  Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#  sort(unique(data$y)), : all z values are equal\n\n#  Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#  sort(unique(data$y)), : Not possible to generate contour data\n#  Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n#  sort(unique(data$y)), : all z values are equal\n#  Warning: Not possible to generate contour data", 
            "title": "Example run"
        }, 
        {
            "location": "/in_depth_introduction/index.html", 
            "text": "Introduction\n\n\nThe first step of MBO requires an initial set of points which are then evaluated by the black box function.\n\n\nThe procedure of MBO is a loop of the following steps:\n\n\n\n\nA user defined surrogate model is fitted on the evaluated points\n\n\nA new evaluation point is proposed by an infill criterion \n\n\nIts performance is evaluated\n\n\n\n\nWe take a look at the already \nquickstart\n example of optimizing the one dimensional Rastrigin function.\n\n\nlibrary(mlrMBO)\n#\n Loading required package: lhs\n#\n Loading required package: smoof\n#\n Loading required package: checkmate\n#\n \n#\n Attaching package: 'smoof'\n#\n The following object is masked from 'package:mlr':\n#\n \n#\n     getParamSet\n#\n Warning: replacing previous import by 'smoof::getParamSet' when loading\n#\n 'mlrMBO'\n#\n \n#\n Attaching package: 'mlrMBO'\n#\n The following object is masked from 'package:ParamHelpers':\n#\n \n#\n     plotEAF\nobj.fun = makeRastriginFunction(1)\n\nlearner = makeLearner(\nregr.km\n, predict.type = \nse\n, covtype = \nmatern3_2\n)\ncontrol = makeMBOControl()\ncontrol = setMBOControlTermination(control, iters = 5)\ncontrol = setMBOControlInfill(control, crit = \nei\n, opt = \nea\n)\n\nresult = mbo(obj.fun, learner = learner, control = control, show.info = TRUE)\n#\n Computing y column(s) for design. Not provided.\n#\n [mbo] 0: x=3.29 : y = 23.5 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-3.22 : y = 18.5 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-0.777 : y = 8.9 : 0.0 secs : initdesign\n#\n [mbo] 0: x=1.93 : y = 4.59 : 0.0 secs : initdesign\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  13.02456 \n#\n   - best initial criterion value(s) :  -14.00775 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       14.008  |proj g|=      0.72708\n#\n At iterate     1  f =       13.729  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 13.7295\n#\n \n#\n F = 13.7295\n#\n final  value 13.729484 \n#\n converged\n#\n Loading required package: emoa\n#\n \n#\n Attaching package: 'emoa'\n#\n The following object is masked from 'package:BBmisc':\n#\n \n#\n     coalesce\n#\n [mbo] 1: x=3.5 : y = 32.2 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  13.4358 \n#\n   - best initial criterion value(s) :  -18.15923 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       18.159  |proj g|=      0.24804\n#\n At iterate     1  f =       18.157  |proj g|=      0.019406\n#\n At iterate     2  f =       18.157  |proj g|=     0.0010016\n#\n At iterate     3  f =       18.157  |proj g|=    4.3897e-06\n#\n \n#\n iterations 3\n#\n function evaluations 5\n#\n segments explored during Cauchy searches 3\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 4.3897e-06\n#\n final function value 18.157\n#\n \n#\n F = 18.157\n#\n final  value 18.156974 \n#\n converged\n#\n [mbo] 2: x=1.76 : y = 12.7 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  13.4358 \n#\n   - best initial criterion value(s) :  -21.29922 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       21.299  |proj g|=      0.33594\n#\n At iterate     1  f =       21.239  |proj g|=        0.2966\n#\n At iterate     2  f =       21.215  |proj g|=       0.76838\n#\n At iterate     3  f =       21.206  |proj g|=       0.14353\n#\n At iterate     4  f =       21.206  |proj g|=      0.013429\n#\n At iterate     5  f =       21.206  |proj g|=    0.00027265\n#\n At iterate     6  f =       21.206  |proj g|=    5.0258e-07\n#\n \n#\n iterations 6\n#\n function evaluations 8\n#\n segments explored during Cauchy searches 6\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 5.02585e-07\n#\n final function value 21.2059\n#\n \n#\n F = 21.2059\n#\n final  value 21.205949 \n#\n converged\n#\n [mbo] 3: x=2.06 : y = 4.87 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  13.4358 \n#\n   - best initial criterion value(s) :  -28.29369 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       28.294  |proj g|=       1.6315\n#\n At iterate     1  f =       25.707  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 25.7075\n#\n \n#\n F = 25.7075\n#\n final  value 25.707464 \n#\n converged\n#\n [mbo] 4: x=1.49 : y = 22.2 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  13.4358 \n#\n   - best initial criterion value(s) :  -26.77505 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       26.775  |proj g|=      0.24803\n#\n At iterate     1  f =       26.774  |proj g|=      0.020612\n#\n At iterate     2  f =       26.774  |proj g|=    0.00051553\n#\n At iterate     3  f =       26.774  |proj g|=      1.11e-06\n#\n \n#\n iterations 3\n#\n function evaluations 5\n#\n segments explored during Cauchy searches 3\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 1.10996e-06\n#\n final function value 26.7739\n#\n \n#\n F = 26.7739\n#\n final  value 26.773908 \n#\n converged\n#\n [mbo] 5: x=-1.13 : y = 4.61 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  13.4358 \n#\n   - best initial criterion value(s) :  -30.8632 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       30.863  |proj g|=       7.3933\n#\n At iterate     1  f =       30.725  |proj g|=       0.64711\n#\n At iterate     2  f =       30.376  |proj g|=       0.51843\n#\n At iterate     3  f =       30.264  |proj g|=       0.11344\n#\n At iterate     4  f =       30.264  |proj g|=      0.034592\n#\n At iterate     5  f =       30.264  |proj g|=     0.0004087\n#\n At iterate     6  f =       30.264  |proj g|=    1.4475e-06\n#\n \n#\n iterations 6\n#\n function evaluations 10\n#\n segments explored during Cauchy searches 6\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 1.44747e-06\n#\n final function value 30.2635\n#\n \n#\n F = 30.2635\n#\n final  value 30.263536 \n#\n converged\n\n\n\n\nFrom this example we can easily see some \nmlrMBO\n essentials, like parameters, learners and the control object.\n\n\nBasically the following steps are needed to start a surrogate-based optimization with our package. \nEach step ends with an R object, which is then passed to \nmbo()\n, i.e., to the working horse of mlrMBO.\n\n\n\n\ndefine the objective function and its parameters by using the package \nsmoof\n\n\ngenerate an initial design\n\n\ndefine a learner, i.e., the surrogate model\n\n\nset up a MBO control object\n\n\nfinally start the optimization\n\n\n\n\nThis web page will provide you with an in-depth introduction on how to set the \nmbo()\n parameters for different kind of optimizations.\n\n\nObjective Function\n\n\nThe first argument of \nmbo()\n is the the objective function created with \nmakeSingleObjectiveFunction\n (or \nmakeMultiObjectiveFunction\n) from the package \nsmoof\n. \n\n\nThroughout this tutorial we demonstrate the optimization of two simple functions: \n\n\n\n\n\n\nobjfun1\n: The 5 dimensional \nackley function\n, which depends on 5 numeric parameters. \nobjfun1\n should be minimized.\n\n\n\n\n\n\nobjfun2\n: A self-constructed sine und cosine combination, with two numeric and 1 categorical parameters. \nobjfun2\n should be maximized. \n\n\n\n\n\n\nThe 5 dimensional \nackley function\n can be generated by the appropriate function of the \nsmoof\n package\n\n\nobjfun1 = makeAckleyFunction(5)\nobjfun1(c(1.8, 2.2, -4, 4, -5))\n#\n [1] 10.9365\n\n\n\n\nThe self-constructed function can be built with \nmakeSingleObjetiveFunction\n. The \npar.set\n argument has to be a ParamSet object from the \nParamHelpers\n package, which provides information about the parameters of the objective function and their constraints for optimization.\nWe define \nj\n in the interval [0,1] and \nk\n as an integer in {1, 2}. The Parameter \nmethod\n is categorical and can be either \n\"a\"\n or \n\"b\"\n.\nAs stated, in this case we want to maximize the function. To do so we have to set \nminimize = FALSE\n.\n\n\nfoo = function(x) {\n  j = x[[1]]\n  k = x[[2]]\n  method = x[[3]]\n  perf = ifelse(method == \na\n, k * sin(j) + cos(j),\n               sin(j) + k * cos(j))\n  return(perf)\n}\n\nobjfun2 = makeSingleObjectiveFunction(\n  name = \nexample\n,\n  fn = foo,\n  par.set = makeParamSet(\n    makeNumericParam(\nj\n, lower = 0,upper = 1),\n    makeIntegerParam(\nk\n, lower = 1L, upper = 2L),\n    makeDiscreteParam(\nmethod\n, values = c(\na\n, \nb\n))\n  ),\n  has.simple.signature = FALSE,\n  minimize = FALSE\n)\n\nobjfun2(list(j = 0.5, k = 1L, method = \na\n))\n#\n [1] 1.357008\n\n\n\n\nInitial Design\n\n\nThe second argument of the \nmbo()\n function - \ndesign\n - is the initial design with default setting \nNULL\n.\n\n\nAn easy (and recommended) way to create an initial design is to use the \ngenerateDesign\n function from the \nParamHelpers\n package. If the default settings are used (i.e. \ndesign = NULL\n) a Random Latin Hypercube \nlhs::randomLHS\n design is used with 4 times the number of parameters the objective function has. Other possibilities to generate designs are for example \ngenerateGridDesign\n and \ngenerateRandomDesign\n. \n\n\nNote:\n If special designs are desired (e.g., orthogonal designs), its interface has to be the same as the interface of the \ngenerateDesign\n objects. \n\n\nFor \nobjfun1\n and \nobjfun2\n we create a slightly larger number of initial points than the default suggests. For \nobjfun1\n we use Random Latin Hypercube sampling and for \nobjfun2\n the Maximin Latin Hypercube sampling. The parameters of the sampling design have to be specified in a list and supplied via \nfun.args\n.\n\n\ninit.points1 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun1)))\n\nset.seed(1)\ndesign1 = generateDesign(n = init.points1, par.set = smoof::getParamSet(objfun1), fun = randomLHS, trafo = FALSE)\n\ninit.points2 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun2)))\ndesign2 = generateDesign(n = init.points2, par.set = smoof::getParamSet(objfun2), fun.args = list(k = 3, dup = 4), fun = maximinLHS, trafo = FALSE)\n\n\n\n\nSurrogate Model\n\n\nThe attribute \nlearner\n of the \nmbo()\n function allows us to choose an appropriate surrogate model for the parameter optimization. Different learners can easily created using the \nmakeLearner\n function from the \nmlr\n package.\nList of implemented learners can be seen using the \nlistlearners()\n function or on the \nmlr wiki\n.\n\n\nThe choice of the surrogate model depends on the parameter set of the objective function.\nWhile kriging models (gaussian processes) are advisable if all parameters are numeric, they cannot be used if the objective function contains categorical parameters. If at least one parameter is categorical, random forest models can be used as surrogate models. The default kriging model is from the \nDiceKriging\n package and uses the \nmatern5_2\ncovariance kernel.\nIn our example we consider these two surrogate models:\n\nkriging\n for optimizing of \nobjfun1\n and \nrandom forest\n for \nobjfun2\n.\n\n\nsurr.km = makeLearner(\nregr.km\n, predict.type = \nse\n, covtype = \nmatern3_2\n)\nsurr.rf = makeLearner(\nregr.randomForest\n)\n\n\n\n\nFurther modification of the learner (e.g., in order to get standard error prediction for design points) will be discussed und illustrated in the section \nExperiments and Output\n.\n\n\nMBOControl\n\n\nThe \nMBOControl\n object controls the fitting process and is created with \nmakeMBOControl\n. General control arguments can be set when creating it (e.g. the number of objectives (\nn.objectives\n), the number of points to propose in each iteration (\npropose.points\n), how the final point is proposed (\nfinal.method\n) etc.).\nTo further adapt the optimization, additional control functions are used to define or change settings of the object:  \n\n\nMBOControlInfill\n\n\nWith \nsetMBOControlInfill\n a \nMBOControl\n object can be extended with infill criteria and infill optimizer options.\n\n\nArgument \ncrit\n\n\nOne of the most important questions is to define how the next design points in the sequential loop are chosen. \n5 different possibilities can be set via the \ncrit\n argument in \nsetMBOControlInfill\n:\n\n\n\n\nmean\n: mean response of the surrogate model\n\n\nei\n: expected improvement of the surrogate model\n\n\naei\n: augmented expected improvement, which is especially useful for the noisy functions\n\n\neqi\n: expected quantile improvement\n\n\ncb\n: confidence bound, which is the additive combination of mean response and mean standard error estimation of the surrogate model (response - lambda * standard.error)\n\n\n\n\nThe parameters of the different criteria are set via further arguments (e.g. \ncrit.cb.lambds\n for the lambda parameter if \ncrit = cb\n) \n\n\nArgument \nopt\n\n\nThe argument \nopt\n sets how the next point to evaluate should be proposed given an infill criterion. \nThe possibilities are:\n\n\n\n\nfocussearch\n: Firstly a Latin Hypercube design of size \nopt.focussearch.points\n (default 10000) is sampled in the parameter space (by \nrandomLHS\n) and the design point with the best prediction of the infill criterion is determined. Then, the parameter space is shrunk around the best design point. This process is repeated \nopt.focussearch.maxit\n (default 5) times and the best seen value of the infill criterion is passed back.\n\n\ncmaes\n: The optimal point is found with a covariance matrix adapting evolutionary strategy from the \ncmaes\n package. If the run fails, a random point is generated and a warning is given. Further control arguments can be provided in \nopt.cmaes.control\n as a list. \n\n\nea\n: Use an evolutionary multiobjective optimization algorithm from the package \nemoa\n to determine the best point. The population size mu can be set by  \nopt.ea.mu\n (default value is 10). (mu+1) means that in each population only one child is generated using crossover und mutation operators. The parameters \neta\n and \np\n of the latter two operators can be adjusted via the attributes \nopt.ea.sbx.eta\n, \nopt.ea.sbx.p\n,\nopt.ea.pm.eta\n and \nopt.ea.pm.p\n. The default number of EA iterations is 500 and can be changed by \nopt.ea.maxit\n attribute.\n\n\nnsga2\n: Use the non-dominated sorting genetic algorithm from the package \nnsga2R\n to determine the best point. This algorithm should be used for \nmulti object optimization\n.\n\n\n\n\nAs all four infill optimization strategies do not guarantee to find the global optimum, users can set the number of restarts by the \nopt.restarts\n argument (default value is 1).\nAfter conducting the desired number of restarts the design point with the best infill criterion value is passed back to the MBO loop.\n\n\nNote:\n Only the \nfocussearch\n optimizer is suitable for for categorical parameters in the parameter set.\n\n\nsetMBOControlTermination\n\n\nWith this control function different criteria to stop the fitting process can be specified. You can set multiple different criteria and the first one that is met will terminate the fitting process.\nYou can set: \n\n\n\n\niters\n: The maximum number of iterations\n\n\ntime.budget\n: A maximum running time in seconds with\n\n\ntarget.fun.value\n: A treshold for function evaluation (stop if a evaluation is better than a given value)\n\n\nmax.evals\n: The maximum number of function evaluations\n\n\n\n\nNote:\n You can also easily create your own stopping condition(s).\n\n\nsetMBOControlMultiPoint\n\n\nThis extends a MBO control object with options for multipoint proposal. Multipoint proposal means, that multiple infill points are suggested and evaluated, which is especially useful in parallel batch evaluation. For a detailed introduction, check the \nmulti-point tutorial\n.\n\n\nArgument: \nmethod\n\n\nDefine the method used for multipoint proposals, currently 3 different methods are supported:\n\n\n\n\ncb\n: Proposes multiple points by optimizing the confidence bound criterion \npropose.points\n times. Generally this works the same way as for the single point case, i.e. specify \ninfill.opt\n. The  lambda parameters are drawn from an exp(1)-distribution.\n\n\nmulticrit\n: Use a evolutionary multicriteria optimization. This is a (mu+1) type evolutionary algorithm and runs for \nmulticrit.maxit\n generations. The population size is set to \npropose.points\n.\n\n\ncl\n: Proposes points by the constant liar strategy, which only makes sense if the confidence bound criterion is used as an infill criterion. In the first step the surrugate model is fitted based on the real data and the best point is calculated accordingly. Then, the function value of the best point is simply guessed by the worst seen function evaluation. This lie is used to update the model in order to propose subsequent point. The procedure is applied until the number of points is \npropose.points\n.\n\n\n\n\nsetMBOControlMultiFid\n\n\nAdd multi-fidelity options to the \nMBOControl\n control object. This is useful when certain parameters increase the performance as well as the calculation cost. The idea is to combine the optimization of fast fitting low-fidelity models and more accurate but expensive high-fidelity models. The parameter on which the fidelity depends on is specified as \nparam\n and the order of the values to train the learner with in \nlvls\n. The costs for the different levels can be specified or estimated by a model based on the execution time of the currently evaluated points.  \n\n\nsetMBOControlMultiCrit\n\n\nThis adds multi-criteria optimization specific options to the control object. For details see the tutorial page on \nmulti-criteria optimization\n.\n\n\nThe list of all attributes is provided in the software documentation.\n\n\nExperiments and Output\n\n\nNow we will apply the mbo() function to optimize the two objective functions.\n\n\ncontrol1 = makeMBOControl()\ncontrol1 = setMBOControlInfill(\n  control = control1,\n  crit = \nei\n,\n  opt = \nfocussearch\n\n)\ncontrol1 = setMBOControlTermination(\n  control = control1,\n  iters = 10\n)\n\ncontrol2 = makeMBOControl()\ncontrol2 = setMBOControlInfill(\n  control = control2,\n  crit = \nmean\n,\n  opt = \nfocussearch\n\n)\ncontrol2 = setMBOControlTermination(\n  control = control2,\n  iters = 10\n)\n\n\n\n\nOptimization of objfun1\n\n\nmbo1 = mbo(objfun1, design = design1, learner = surr.km, control = control1, \n           show.info = FALSE)\n\n\n\n\nmbo1\n#\n Recommended parameters:\n#\n x=1.03,-11.1,-2.17,8.79,-0.205\n#\n Objective: y = 15.517\n#\n \n#\n Optimization path\n#\n 25 + 10 entries in total, displaying last 10 (or less):\n#\n              x1        x2         x3        x4          x5        y dob\n#\n 26  -4.12605695 -11.54782 -5.6561831 10.981043  0.97068084 17.24931   1\n#\n 27  -4.08285380 -12.35181 -7.4694699 30.601857  2.35503836 21.12274   2\n#\n 28  -3.63459848 -11.16748 -0.2215530  9.728138 -0.59150240 16.78718   3\n#\n 29   5.57099943 -10.94766  6.4746657  8.850323 -1.46103227 17.39163   4\n#\n 30  -5.86468716 -10.71919 21.1616368  9.390592 -2.57240184 19.94265   5\n#\n 31  17.32988712  -9.19084 -1.3564536  8.747120 -0.67130885 19.02134   6\n#\n 32 -15.19605365 -11.42044 -0.7151403  8.628104 -4.72241544 19.05059   7\n#\n 33  20.40116345 -11.35839  0.1671055  8.996293  0.07495621 19.37707   8\n#\n 34  -0.09598261 -11.03762 -0.2741146  9.177053 -0.53188528 15.93757   9\n#\n 35   1.02545547 -11.14438 -2.1749602  8.793208 -0.20463488 15.51709  10\n#\n    eol error.message exec.time          ei error.model train.time\n#\n 26  NA          \nNA\n     0.000 -0.13572183        \nNA\n      0.044\n#\n 27  NA          \nNA\n     0.001 -0.04343611        \nNA\n      0.050\n#\n 28  NA          \nNA\n     0.000 -0.06930553        \nNA\n      0.066\n#\n 29  NA          \nNA\n     0.000 -0.07111303        \nNA\n      0.089\n#\n 30  NA          \nNA\n     0.000 -0.04882831        \nNA\n      0.062\n#\n 31  NA          \nNA\n     0.000 -0.05262250        \nNA\n      0.057\n#\n 32  NA          \nNA\n     0.000 -0.04465761        \nNA\n      0.064\n#\n 33  NA          \nNA\n     0.000 -0.07328187        \nNA\n      0.117\n#\n 34  NA          \nNA\n     0.000 -0.15297708        \nNA\n      0.042\n#\n 35  NA          \nNA\n     0.000 -0.09966843        \nNA\n      0.131\n#\n    prop.type propose.time exec.timestamp        se     mean\n#\n 26 infill_ei        1.277     1464952490 0.1701271 19.75186\n#\n 27 infill_ei        1.301     1464952492 0.2315570 17.37290\n#\n 28 infill_ei        1.194     1464952493 0.2334914 17.30166\n#\n 29 infill_ei        1.427     1464952495 0.1798618 16.78846\n#\n 30 infill_ei        1.500     1464952497 0.2711088 16.93882\n#\n 31 infill_ei        1.433     1464952499 0.3026688 16.96312\n#\n 32 infill_ei        1.389     1464952500 0.2836455 16.96928\n#\n 33 infill_ei        1.363     1464952502 0.3412791 16.93965\n#\n 34 infill_ei        1.440     1464952504 0.1966018 16.66667\n#\n 35 infill_ei        1.428     1464952506 0.1522894 15.87113\n\n\n\n\nThe default output of mbo contains the best found parameter set and the optimzation path. The \nMBOResult\n object contains additional information, most importanty:\n\n\n\n\nx\n: The best point of the parameter space\n\n\ny\n: The associated best value of the objective function\n\n\nopt.path\n: The optimization path. See \n?ParamHelpers::OptPath\n for further information.\n\n\nmodels\n: Depending on \nstore.model.at\n in the \nMBOControl\n object, this contains zero, one or multiple surrogate models (default is to save the model generated after the last iteration).\n\n\n...\n\n\n\n\nWe can also change some arguments of the \nMBOControl\n object and run \nmbo()\n again:\n\n\ncontrol1$infill.crit = \ncb\n\ncontrol1$iters = 5L\nmbo1 = mbo(objfun1, design = design1, learner = surr.km, control = control1, show.info = FALSE)\n\n\n\n\nmbo1\n#\n Recommended parameters:\n#\n x=0.555,-6.83,6.51,4.35,-1.98\n#\n Objective: y = 14.150\n#\n \n#\n Optimization path\n#\n 25 + 10 entries in total, displaying last 10 (or less):\n#\n            x1         x2        x3         x4         x5        y dob eol\n#\n 26 -4.6202804 -10.307607 16.870109   5.583554  -5.838394 19.02196   1  NA\n#\n 27 -4.6471192 -13.116119 27.086733  17.835397 -10.658748 20.76526   2  NA\n#\n 28 -7.2288110 -10.883809 23.025155   4.371716  -5.586849 19.92917   3  NA\n#\n 29 -4.3568261 -15.236885 26.828895   5.941049  -6.683645 20.52021   4  NA\n#\n 30  4.0754507 -10.199311 13.723630   4.539547 -11.181502 18.64589   5  NA\n#\n 31 22.2615423  -8.420510 12.101165   4.913087 -17.755816 20.45866   6  NA\n#\n 32  6.5055626 -10.197154 10.433297  14.087065 -22.152559 20.46904   7  NA\n#\n 33  0.1582629  -7.663526 12.009755   4.105232  -3.323199 16.26470   8  NA\n#\n 34  0.1181351  -7.649562 11.368898 -11.688711  -2.412261 18.07821   9  NA\n#\n 35  0.5549428  -6.827755  6.507281   4.347422  -1.982138 14.14984  10  NA\n#\n    error.message exec.time       cb error.model train.time prop.type\n#\n 26          \nNA\n     0.001 19.65947        \nNA\n      0.045 infill_cb\n#\n 27          \nNA\n     0.000 18.90914        \nNA\n      0.057 infill_cb\n#\n 28          \nNA\n     0.000 18.92139        \nNA\n      0.047 infill_cb\n#\n 29          \nNA\n     0.000 18.93866        \nNA\n      0.079 infill_cb\n#\n 30          \nNA\n     0.000 18.82714        \nNA\n      0.040 infill_cb\n#\n 31          \nNA\n     0.000 18.48476        \nNA\n      0.065 infill_cb\n#\n 32          \nNA\n     0.001 18.50289        \nNA\n      0.078 infill_cb\n#\n 33          \nNA\n     0.000 18.42454        \nNA\n      0.058 infill_cb\n#\n 34          \nNA\n     0.001 16.11724        \nNA\n      0.064 infill_cb\n#\n 35          \nNA\n     0.000 16.10702        \nNA\n      0.102 infill_cb\n#\n    propose.time exec.timestamp        se     mean lambda\n#\n 26        1.425     1464952508 0.1837239 19.84319      1\n#\n 27        1.300     1464952509 0.1712301 19.08037      1\n#\n 28        1.332     1464952511 0.1575612 19.07895      1\n#\n 29        1.238     1464952513 0.1379985 19.07666      1\n#\n 30        1.344     1464952514 0.1967692 19.02391      1\n#\n 31        1.379     1464952516 0.1810977 18.66586      1\n#\n 32        1.378     1464952518 0.1946998 18.69759      1\n#\n 33        1.367     1464952520 0.2214879 18.64603      1\n#\n 34        1.388     1464952521 0.2650294 16.38227      1\n#\n 35        1.391     1464952523 0.2213947 16.32841      1\n\n\n\n\nOptimization of objfun2\n\n\nNow let us use \nmlrMBO\n to optimize \nobjfun2\n, which contains one categorical variable.\nAs we have already mentioned before, in case of factor variables only \nfocussearch\n is suitable and kriging cannot be used as a surrogate model.\nIf we would use \nmean\n as the infill criterion, any kind of model which can handle factors variables is possible (like regression trees, random forests, linear models and many others).\n\n\nmbo2 = mbo(objfun2, design = design2, learner = surr.rf, control = control2, show.info = FALSE)\n\n\n\n\nmbo2\n#\n Recommended parameters:\n#\n j=0.462; k=2; method=b\n#\n Objective: y = 2.236\n#\n \n#\n Optimization path\n#\n 15 + 10 entries in total, displaying last 10 (or less):\n#\n            j k method        y dob eol error.message exec.time      mean\n#\n 16 0.4757427 2      b 2.235904   1  NA          \nNA\n     0.000 -2.026581\n#\n 17 0.4849609 2      b 2.235560   2  NA          \nNA\n     0.000 -2.082962\n#\n 18 0.4717123 2      b 2.235995   3  NA          \nNA\n     0.000 -2.119596\n#\n 19 0.4777613 2      b 2.235845   4  NA          \nNA\n     0.000 -2.138953\n#\n 20 0.4674350 2      b 2.236052   5  NA          \nNA\n     0.001 -2.152694\n#\n 21 0.4660672 2      b 2.236061   6  NA          \nNA\n     0.000 -2.161759\n#\n 22 0.4622545 2      b 2.236066   7  NA          \nNA\n     0.000 -2.172939\n#\n 23 0.4673248 2      b 2.236053   8  NA          \nNA\n     0.000 -2.174135\n#\n 24 0.4533910 2      b 2.235950   9  NA          \nNA\n     0.000 -2.175038\n#\n 25 0.4587362 2      b 2.236041  10  NA          \nNA\n     0.000 -2.187947\n#\n    error.model train.time   prop.type propose.time exec.timestamp\n#\n 16        \nNA\n      0.007 infill_mean        0.496     1464952524\n#\n 17        \nNA\n      0.006 infill_mean        0.507     1464952525\n#\n 18        \nNA\n      0.006 infill_mean        0.511     1464952525\n#\n 19        \nNA\n      0.006 infill_mean        0.520     1464952526\n#\n 20        \nNA\n      0.006 infill_mean        0.544     1464952527\n#\n 21        \nNA\n      0.007 infill_mean        0.547     1464952528\n#\n 22        \nNA\n      0.008 infill_mean        0.524     1464952529\n#\n 23        \nNA\n      0.007 infill_mean        0.566     1464952530\n#\n 24        \nNA\n      0.007 infill_mean        0.594     1464952531\n#\n 25        \nNA\n      0.007 infill_mean        0.622     1464952532\n\n\n\n\nIf we want to use the expected improvement \nei\n or (lower) confidence bound \ncb\n infill criteria, the \npredict.type\n attribute of the learner has be set to \nse\n. A list of regression learners which support it can be viewed by:\n\n\nlistLearners(obj = \nregr\n, properties = \nse\n)\n\n\n\n\nWe modify the random forest to predict the standard error and optimize \nobjfun2\n by the \nei\n infill criterion.\n\n\nlearner_rf = makeLearner(\nregr.randomForest\n, predict.type = \nse\n)\ncontrol2$infill.crit = \nei\n\nmbo2 = mbo(objfun2, design = design2, learner = learner_rf, \n           control = control2, show.info = FALSE)\n\n\n\n\nmbo2\n#\n Recommended parameters:\n#\n j=0.428; k=2; method=b\n#\n Objective: y = 2.235\n#\n \n#\n Optimization path\n#\n 15 + 10 entries in total, displaying last 10 (or less):\n#\n            j k method        y dob eol error.message exec.time\n#\n 16 0.4087665 2      b 2.232701   1  NA          \nNA\n     0.000\n#\n 17 0.4087998 2      b 2.232705   2  NA          \nNA\n     0.000\n#\n 18 0.4033413 2      b 2.232003   3  NA          \nNA\n     0.001\n#\n 19 0.3865188 2      b 2.229420   4  NA          \nNA\n     0.000\n#\n 20 0.3744102 2      b 2.227171   5  NA          \nNA\n     0.000\n#\n 21 0.3983756 2      b 2.231306   6  NA          \nNA\n     0.000\n#\n 22 0.3475329 2      b 2.221011   7  NA          \nNA\n     0.000\n#\n 23 0.3283410 2      b 2.215630   8  NA          \nNA\n     0.000\n#\n 24 0.3200959 2      b 2.213068   9  NA          \nNA\n     0.000\n#\n 25 0.3247575 2      b 2.214535  10  NA          \nNA\n     0.000\n#\n              ei error.model train.time prop.type propose.time\n#\n 16 -0.001032499        \nNA\n      0.177 infill_ei       33.625\n#\n 17 -0.003238726        \nNA\n      0.168 infill_ei       31.749\n#\n 18 -0.002051443        \nNA\n      0.201 infill_ei       31.214\n#\n 19 -0.002915200        \nNA\n      0.183 infill_ei       31.944\n#\n 20 -0.002450279        \nNA\n      0.173 infill_ei       31.415\n#\n 21 -0.002619635        \nNA\n      0.175 infill_ei       31.749\n#\n 22 -0.004576781        \nNA\n      0.178 infill_ei       32.058\n#\n 23 -0.005052992        \nNA\n      0.180 infill_ei       31.290\n#\n 24 -0.003741054        \nNA\n      0.180 infill_ei       31.704\n#\n 25 -0.004406909        \nNA\n      0.194 infill_ei       32.131\n#\n    exec.timestamp         se     mean\n#\n 16     1464952566 0.14104875 1.944803\n#\n 17     1464952599 0.14306666 2.004142\n#\n 18     1464952630 0.10511671 2.058761\n#\n 19     1464952663 0.13876233 2.006720\n#\n 20     1464952695 0.09833344 2.080229\n#\n 21     1464952727 0.08121638 2.116259\n#\n 22     1464952760 0.10829462 2.090093\n#\n 23     1464952792 0.12091591 2.072615\n#\n 24     1464952824 0.10710490 2.082343\n#\n 25     1464952857 0.08455944 2.130153\n\n\n\n\nFinally, if a learner, which does not support the \nse\n prediction type, should be applied for the optimization with the \nei\n infill criterion, it is  possible to create a bagging model with the desired characteristics. For details on how to do it take a look at the \nbagging section\n in the \nmlr\n tutorial.", 
            "title": "In Depth Introduction"
        }, 
        {
            "location": "/in_depth_introduction/index.html#introduction", 
            "text": "The first step of MBO requires an initial set of points which are then evaluated by the black box function.  The procedure of MBO is a loop of the following steps:   A user defined surrogate model is fitted on the evaluated points  A new evaluation point is proposed by an infill criterion   Its performance is evaluated   We take a look at the already  quickstart  example of optimizing the one dimensional Rastrigin function.  library(mlrMBO)\n#  Loading required package: lhs\n#  Loading required package: smoof\n#  Loading required package: checkmate\n#  \n#  Attaching package: 'smoof'\n#  The following object is masked from 'package:mlr':\n#  \n#      getParamSet\n#  Warning: replacing previous import by 'smoof::getParamSet' when loading\n#  'mlrMBO'\n#  \n#  Attaching package: 'mlrMBO'\n#  The following object is masked from 'package:ParamHelpers':\n#  \n#      plotEAF\nobj.fun = makeRastriginFunction(1)\n\nlearner = makeLearner( regr.km , predict.type =  se , covtype =  matern3_2 )\ncontrol = makeMBOControl()\ncontrol = setMBOControlTermination(control, iters = 5)\ncontrol = setMBOControlInfill(control, crit =  ei , opt =  ea )\n\nresult = mbo(obj.fun, learner = learner, control = control, show.info = TRUE)\n#  Computing y column(s) for design. Not provided.\n#  [mbo] 0: x=3.29 : y = 23.5 : 0.0 secs : initdesign\n#  [mbo] 0: x=-3.22 : y = 18.5 : 0.0 secs : initdesign\n#  [mbo] 0: x=-0.777 : y = 8.9 : 0.0 secs : initdesign\n#  [mbo] 0: x=1.93 : y = 4.59 : 0.0 secs : initdesign\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  13.02456 \n#    - best initial criterion value(s) :  -14.00775 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       14.008  |proj g|=      0.72708\n#  At iterate     1  f =       13.729  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 13.7295\n#  \n#  F = 13.7295\n#  final  value 13.729484 \n#  converged\n#  Loading required package: emoa\n#  \n#  Attaching package: 'emoa'\n#  The following object is masked from 'package:BBmisc':\n#  \n#      coalesce\n#  [mbo] 1: x=3.5 : y = 32.2 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  13.4358 \n#    - best initial criterion value(s) :  -18.15923 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       18.159  |proj g|=      0.24804\n#  At iterate     1  f =       18.157  |proj g|=      0.019406\n#  At iterate     2  f =       18.157  |proj g|=     0.0010016\n#  At iterate     3  f =       18.157  |proj g|=    4.3897e-06\n#  \n#  iterations 3\n#  function evaluations 5\n#  segments explored during Cauchy searches 3\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 0\n#  norm of the final projected gradient 4.3897e-06\n#  final function value 18.157\n#  \n#  F = 18.157\n#  final  value 18.156974 \n#  converged\n#  [mbo] 2: x=1.76 : y = 12.7 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  13.4358 \n#    - best initial criterion value(s) :  -21.29922 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       21.299  |proj g|=      0.33594\n#  At iterate     1  f =       21.239  |proj g|=        0.2966\n#  At iterate     2  f =       21.215  |proj g|=       0.76838\n#  At iterate     3  f =       21.206  |proj g|=       0.14353\n#  At iterate     4  f =       21.206  |proj g|=      0.013429\n#  At iterate     5  f =       21.206  |proj g|=    0.00027265\n#  At iterate     6  f =       21.206  |proj g|=    5.0258e-07\n#  \n#  iterations 6\n#  function evaluations 8\n#  segments explored during Cauchy searches 6\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 0\n#  norm of the final projected gradient 5.02585e-07\n#  final function value 21.2059\n#  \n#  F = 21.2059\n#  final  value 21.205949 \n#  converged\n#  [mbo] 3: x=2.06 : y = 4.87 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  13.4358 \n#    - best initial criterion value(s) :  -28.29369 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       28.294  |proj g|=       1.6315\n#  At iterate     1  f =       25.707  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 25.7075\n#  \n#  F = 25.7075\n#  final  value 25.707464 \n#  converged\n#  [mbo] 4: x=1.49 : y = 22.2 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  13.4358 \n#    - best initial criterion value(s) :  -26.77505 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       26.775  |proj g|=      0.24803\n#  At iterate     1  f =       26.774  |proj g|=      0.020612\n#  At iterate     2  f =       26.774  |proj g|=    0.00051553\n#  At iterate     3  f =       26.774  |proj g|=      1.11e-06\n#  \n#  iterations 3\n#  function evaluations 5\n#  segments explored during Cauchy searches 3\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 0\n#  norm of the final projected gradient 1.10996e-06\n#  final function value 26.7739\n#  \n#  F = 26.7739\n#  final  value 26.773908 \n#  converged\n#  [mbo] 5: x=-1.13 : y = 4.61 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  13.4358 \n#    - best initial criterion value(s) :  -30.8632 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       30.863  |proj g|=       7.3933\n#  At iterate     1  f =       30.725  |proj g|=       0.64711\n#  At iterate     2  f =       30.376  |proj g|=       0.51843\n#  At iterate     3  f =       30.264  |proj g|=       0.11344\n#  At iterate     4  f =       30.264  |proj g|=      0.034592\n#  At iterate     5  f =       30.264  |proj g|=     0.0004087\n#  At iterate     6  f =       30.264  |proj g|=    1.4475e-06\n#  \n#  iterations 6\n#  function evaluations 10\n#  segments explored during Cauchy searches 6\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 0\n#  norm of the final projected gradient 1.44747e-06\n#  final function value 30.2635\n#  \n#  F = 30.2635\n#  final  value 30.263536 \n#  converged  From this example we can easily see some  mlrMBO  essentials, like parameters, learners and the control object.  Basically the following steps are needed to start a surrogate-based optimization with our package. \nEach step ends with an R object, which is then passed to  mbo() , i.e., to the working horse of mlrMBO.   define the objective function and its parameters by using the package  smoof  generate an initial design  define a learner, i.e., the surrogate model  set up a MBO control object  finally start the optimization   This web page will provide you with an in-depth introduction on how to set the  mbo()  parameters for different kind of optimizations.", 
            "title": "Introduction"
        }, 
        {
            "location": "/in_depth_introduction/index.html#objective-function", 
            "text": "The first argument of  mbo()  is the the objective function created with  makeSingleObjectiveFunction  (or  makeMultiObjectiveFunction ) from the package  smoof .   Throughout this tutorial we demonstrate the optimization of two simple functions:     objfun1 : The 5 dimensional  ackley function , which depends on 5 numeric parameters.  objfun1  should be minimized.    objfun2 : A self-constructed sine und cosine combination, with two numeric and 1 categorical parameters.  objfun2  should be maximized.     The 5 dimensional  ackley function  can be generated by the appropriate function of the  smoof  package  objfun1 = makeAckleyFunction(5)\nobjfun1(c(1.8, 2.2, -4, 4, -5))\n#  [1] 10.9365  The self-constructed function can be built with  makeSingleObjetiveFunction . The  par.set  argument has to be a ParamSet object from the  ParamHelpers  package, which provides information about the parameters of the objective function and their constraints for optimization.\nWe define  j  in the interval [0,1] and  k  as an integer in {1, 2}. The Parameter  method  is categorical and can be either  \"a\"  or  \"b\" .\nAs stated, in this case we want to maximize the function. To do so we have to set  minimize = FALSE .  foo = function(x) {\n  j = x[[1]]\n  k = x[[2]]\n  method = x[[3]]\n  perf = ifelse(method ==  a , k * sin(j) + cos(j),\n               sin(j) + k * cos(j))\n  return(perf)\n}\n\nobjfun2 = makeSingleObjectiveFunction(\n  name =  example ,\n  fn = foo,\n  par.set = makeParamSet(\n    makeNumericParam( j , lower = 0,upper = 1),\n    makeIntegerParam( k , lower = 1L, upper = 2L),\n    makeDiscreteParam( method , values = c( a ,  b ))\n  ),\n  has.simple.signature = FALSE,\n  minimize = FALSE\n)\n\nobjfun2(list(j = 0.5, k = 1L, method =  a ))\n#  [1] 1.357008", 
            "title": "Objective Function"
        }, 
        {
            "location": "/in_depth_introduction/index.html#initial-design", 
            "text": "The second argument of the  mbo()  function -  design  - is the initial design with default setting  NULL .  An easy (and recommended) way to create an initial design is to use the  generateDesign  function from the  ParamHelpers  package. If the default settings are used (i.e.  design = NULL ) a Random Latin Hypercube  lhs::randomLHS  design is used with 4 times the number of parameters the objective function has. Other possibilities to generate designs are for example  generateGridDesign  and  generateRandomDesign .   Note:  If special designs are desired (e.g., orthogonal designs), its interface has to be the same as the interface of the  generateDesign  objects.   For  objfun1  and  objfun2  we create a slightly larger number of initial points than the default suggests. For  objfun1  we use Random Latin Hypercube sampling and for  objfun2  the Maximin Latin Hypercube sampling. The parameters of the sampling design have to be specified in a list and supplied via  fun.args .  init.points1 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun1)))\n\nset.seed(1)\ndesign1 = generateDesign(n = init.points1, par.set = smoof::getParamSet(objfun1), fun = randomLHS, trafo = FALSE)\n\ninit.points2 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun2)))\ndesign2 = generateDesign(n = init.points2, par.set = smoof::getParamSet(objfun2), fun.args = list(k = 3, dup = 4), fun = maximinLHS, trafo = FALSE)", 
            "title": "Initial Design"
        }, 
        {
            "location": "/in_depth_introduction/index.html#surrogate-model", 
            "text": "The attribute  learner  of the  mbo()  function allows us to choose an appropriate surrogate model for the parameter optimization. Different learners can easily created using the  makeLearner  function from the  mlr  package.\nList of implemented learners can be seen using the  listlearners()  function or on the  mlr wiki .  The choice of the surrogate model depends on the parameter set of the objective function.\nWhile kriging models (gaussian processes) are advisable if all parameters are numeric, they cannot be used if the objective function contains categorical parameters. If at least one parameter is categorical, random forest models can be used as surrogate models. The default kriging model is from the  DiceKriging  package and uses the  matern5_2 covariance kernel.\nIn our example we consider these two surrogate models: kriging  for optimizing of  objfun1  and  random forest  for  objfun2 .  surr.km = makeLearner( regr.km , predict.type =  se , covtype =  matern3_2 )\nsurr.rf = makeLearner( regr.randomForest )  Further modification of the learner (e.g., in order to get standard error prediction for design points) will be discussed und illustrated in the section  Experiments and Output .", 
            "title": "Surrogate Model"
        }, 
        {
            "location": "/in_depth_introduction/index.html#mbocontrol", 
            "text": "The  MBOControl  object controls the fitting process and is created with  makeMBOControl . General control arguments can be set when creating it (e.g. the number of objectives ( n.objectives ), the number of points to propose in each iteration ( propose.points ), how the final point is proposed ( final.method ) etc.).\nTo further adapt the optimization, additional control functions are used to define or change settings of the object:", 
            "title": "MBOControl"
        }, 
        {
            "location": "/in_depth_introduction/index.html#mbocontrolinfill", 
            "text": "With  setMBOControlInfill  a  MBOControl  object can be extended with infill criteria and infill optimizer options.  Argument  crit  One of the most important questions is to define how the next design points in the sequential loop are chosen. \n5 different possibilities can be set via the  crit  argument in  setMBOControlInfill :   mean : mean response of the surrogate model  ei : expected improvement of the surrogate model  aei : augmented expected improvement, which is especially useful for the noisy functions  eqi : expected quantile improvement  cb : confidence bound, which is the additive combination of mean response and mean standard error estimation of the surrogate model (response - lambda * standard.error)   The parameters of the different criteria are set via further arguments (e.g.  crit.cb.lambds  for the lambda parameter if  crit = cb )   Argument  opt  The argument  opt  sets how the next point to evaluate should be proposed given an infill criterion. \nThe possibilities are:   focussearch : Firstly a Latin Hypercube design of size  opt.focussearch.points  (default 10000) is sampled in the parameter space (by  randomLHS ) and the design point with the best prediction of the infill criterion is determined. Then, the parameter space is shrunk around the best design point. This process is repeated  opt.focussearch.maxit  (default 5) times and the best seen value of the infill criterion is passed back.  cmaes : The optimal point is found with a covariance matrix adapting evolutionary strategy from the  cmaes  package. If the run fails, a random point is generated and a warning is given. Further control arguments can be provided in  opt.cmaes.control  as a list.   ea : Use an evolutionary multiobjective optimization algorithm from the package  emoa  to determine the best point. The population size mu can be set by   opt.ea.mu  (default value is 10). (mu+1) means that in each population only one child is generated using crossover und mutation operators. The parameters  eta  and  p  of the latter two operators can be adjusted via the attributes  opt.ea.sbx.eta ,  opt.ea.sbx.p , opt.ea.pm.eta  and  opt.ea.pm.p . The default number of EA iterations is 500 and can be changed by  opt.ea.maxit  attribute.  nsga2 : Use the non-dominated sorting genetic algorithm from the package  nsga2R  to determine the best point. This algorithm should be used for  multi object optimization .   As all four infill optimization strategies do not guarantee to find the global optimum, users can set the number of restarts by the  opt.restarts  argument (default value is 1).\nAfter conducting the desired number of restarts the design point with the best infill criterion value is passed back to the MBO loop.  Note:  Only the  focussearch  optimizer is suitable for for categorical parameters in the parameter set.", 
            "title": "MBOControlInfill"
        }, 
        {
            "location": "/in_depth_introduction/index.html#setmbocontroltermination", 
            "text": "With this control function different criteria to stop the fitting process can be specified. You can set multiple different criteria and the first one that is met will terminate the fitting process.\nYou can set:    iters : The maximum number of iterations  time.budget : A maximum running time in seconds with  target.fun.value : A treshold for function evaluation (stop if a evaluation is better than a given value)  max.evals : The maximum number of function evaluations   Note:  You can also easily create your own stopping condition(s).", 
            "title": "setMBOControlTermination"
        }, 
        {
            "location": "/in_depth_introduction/index.html#setmbocontrolmultipoint", 
            "text": "This extends a MBO control object with options for multipoint proposal. Multipoint proposal means, that multiple infill points are suggested and evaluated, which is especially useful in parallel batch evaluation. For a detailed introduction, check the  multi-point tutorial .  Argument:  method  Define the method used for multipoint proposals, currently 3 different methods are supported:   cb : Proposes multiple points by optimizing the confidence bound criterion  propose.points  times. Generally this works the same way as for the single point case, i.e. specify  infill.opt . The  lambda parameters are drawn from an exp(1)-distribution.  multicrit : Use a evolutionary multicriteria optimization. This is a (mu+1) type evolutionary algorithm and runs for  multicrit.maxit  generations. The population size is set to  propose.points .  cl : Proposes points by the constant liar strategy, which only makes sense if the confidence bound criterion is used as an infill criterion. In the first step the surrugate model is fitted based on the real data and the best point is calculated accordingly. Then, the function value of the best point is simply guessed by the worst seen function evaluation. This lie is used to update the model in order to propose subsequent point. The procedure is applied until the number of points is  propose.points .", 
            "title": "setMBOControlMultiPoint"
        }, 
        {
            "location": "/in_depth_introduction/index.html#setmbocontrolmultifid", 
            "text": "Add multi-fidelity options to the  MBOControl  control object. This is useful when certain parameters increase the performance as well as the calculation cost. The idea is to combine the optimization of fast fitting low-fidelity models and more accurate but expensive high-fidelity models. The parameter on which the fidelity depends on is specified as  param  and the order of the values to train the learner with in  lvls . The costs for the different levels can be specified or estimated by a model based on the execution time of the currently evaluated points.", 
            "title": "setMBOControlMultiFid"
        }, 
        {
            "location": "/in_depth_introduction/index.html#setmbocontrolmulticrit", 
            "text": "This adds multi-criteria optimization specific options to the control object. For details see the tutorial page on  multi-criteria optimization .  The list of all attributes is provided in the software documentation.", 
            "title": "setMBOControlMultiCrit"
        }, 
        {
            "location": "/in_depth_introduction/index.html#experiments-and-output", 
            "text": "Now we will apply the mbo() function to optimize the two objective functions.  control1 = makeMBOControl()\ncontrol1 = setMBOControlInfill(\n  control = control1,\n  crit =  ei ,\n  opt =  focussearch \n)\ncontrol1 = setMBOControlTermination(\n  control = control1,\n  iters = 10\n)\n\ncontrol2 = makeMBOControl()\ncontrol2 = setMBOControlInfill(\n  control = control2,\n  crit =  mean ,\n  opt =  focussearch \n)\ncontrol2 = setMBOControlTermination(\n  control = control2,\n  iters = 10\n)", 
            "title": "Experiments and Output"
        }, 
        {
            "location": "/in_depth_introduction/index.html#optimization-of-objfun1", 
            "text": "mbo1 = mbo(objfun1, design = design1, learner = surr.km, control = control1, \n           show.info = FALSE)  mbo1\n#  Recommended parameters:\n#  x=1.03,-11.1,-2.17,8.79,-0.205\n#  Objective: y = 15.517\n#  \n#  Optimization path\n#  25 + 10 entries in total, displaying last 10 (or less):\n#               x1        x2         x3        x4          x5        y dob\n#  26  -4.12605695 -11.54782 -5.6561831 10.981043  0.97068084 17.24931   1\n#  27  -4.08285380 -12.35181 -7.4694699 30.601857  2.35503836 21.12274   2\n#  28  -3.63459848 -11.16748 -0.2215530  9.728138 -0.59150240 16.78718   3\n#  29   5.57099943 -10.94766  6.4746657  8.850323 -1.46103227 17.39163   4\n#  30  -5.86468716 -10.71919 21.1616368  9.390592 -2.57240184 19.94265   5\n#  31  17.32988712  -9.19084 -1.3564536  8.747120 -0.67130885 19.02134   6\n#  32 -15.19605365 -11.42044 -0.7151403  8.628104 -4.72241544 19.05059   7\n#  33  20.40116345 -11.35839  0.1671055  8.996293  0.07495621 19.37707   8\n#  34  -0.09598261 -11.03762 -0.2741146  9.177053 -0.53188528 15.93757   9\n#  35   1.02545547 -11.14438 -2.1749602  8.793208 -0.20463488 15.51709  10\n#     eol error.message exec.time          ei error.model train.time\n#  26  NA           NA      0.000 -0.13572183         NA       0.044\n#  27  NA           NA      0.001 -0.04343611         NA       0.050\n#  28  NA           NA      0.000 -0.06930553         NA       0.066\n#  29  NA           NA      0.000 -0.07111303         NA       0.089\n#  30  NA           NA      0.000 -0.04882831         NA       0.062\n#  31  NA           NA      0.000 -0.05262250         NA       0.057\n#  32  NA           NA      0.000 -0.04465761         NA       0.064\n#  33  NA           NA      0.000 -0.07328187         NA       0.117\n#  34  NA           NA      0.000 -0.15297708         NA       0.042\n#  35  NA           NA      0.000 -0.09966843         NA       0.131\n#     prop.type propose.time exec.timestamp        se     mean\n#  26 infill_ei        1.277     1464952490 0.1701271 19.75186\n#  27 infill_ei        1.301     1464952492 0.2315570 17.37290\n#  28 infill_ei        1.194     1464952493 0.2334914 17.30166\n#  29 infill_ei        1.427     1464952495 0.1798618 16.78846\n#  30 infill_ei        1.500     1464952497 0.2711088 16.93882\n#  31 infill_ei        1.433     1464952499 0.3026688 16.96312\n#  32 infill_ei        1.389     1464952500 0.2836455 16.96928\n#  33 infill_ei        1.363     1464952502 0.3412791 16.93965\n#  34 infill_ei        1.440     1464952504 0.1966018 16.66667\n#  35 infill_ei        1.428     1464952506 0.1522894 15.87113  The default output of mbo contains the best found parameter set and the optimzation path. The  MBOResult  object contains additional information, most importanty:   x : The best point of the parameter space  y : The associated best value of the objective function  opt.path : The optimization path. See  ?ParamHelpers::OptPath  for further information.  models : Depending on  store.model.at  in the  MBOControl  object, this contains zero, one or multiple surrogate models (default is to save the model generated after the last iteration).  ...   We can also change some arguments of the  MBOControl  object and run  mbo()  again:  control1$infill.crit =  cb \ncontrol1$iters = 5L\nmbo1 = mbo(objfun1, design = design1, learner = surr.km, control = control1, show.info = FALSE)  mbo1\n#  Recommended parameters:\n#  x=0.555,-6.83,6.51,4.35,-1.98\n#  Objective: y = 14.150\n#  \n#  Optimization path\n#  25 + 10 entries in total, displaying last 10 (or less):\n#             x1         x2        x3         x4         x5        y dob eol\n#  26 -4.6202804 -10.307607 16.870109   5.583554  -5.838394 19.02196   1  NA\n#  27 -4.6471192 -13.116119 27.086733  17.835397 -10.658748 20.76526   2  NA\n#  28 -7.2288110 -10.883809 23.025155   4.371716  -5.586849 19.92917   3  NA\n#  29 -4.3568261 -15.236885 26.828895   5.941049  -6.683645 20.52021   4  NA\n#  30  4.0754507 -10.199311 13.723630   4.539547 -11.181502 18.64589   5  NA\n#  31 22.2615423  -8.420510 12.101165   4.913087 -17.755816 20.45866   6  NA\n#  32  6.5055626 -10.197154 10.433297  14.087065 -22.152559 20.46904   7  NA\n#  33  0.1582629  -7.663526 12.009755   4.105232  -3.323199 16.26470   8  NA\n#  34  0.1181351  -7.649562 11.368898 -11.688711  -2.412261 18.07821   9  NA\n#  35  0.5549428  -6.827755  6.507281   4.347422  -1.982138 14.14984  10  NA\n#     error.message exec.time       cb error.model train.time prop.type\n#  26           NA      0.001 19.65947         NA       0.045 infill_cb\n#  27           NA      0.000 18.90914         NA       0.057 infill_cb\n#  28           NA      0.000 18.92139         NA       0.047 infill_cb\n#  29           NA      0.000 18.93866         NA       0.079 infill_cb\n#  30           NA      0.000 18.82714         NA       0.040 infill_cb\n#  31           NA      0.000 18.48476         NA       0.065 infill_cb\n#  32           NA      0.001 18.50289         NA       0.078 infill_cb\n#  33           NA      0.000 18.42454         NA       0.058 infill_cb\n#  34           NA      0.001 16.11724         NA       0.064 infill_cb\n#  35           NA      0.000 16.10702         NA       0.102 infill_cb\n#     propose.time exec.timestamp        se     mean lambda\n#  26        1.425     1464952508 0.1837239 19.84319      1\n#  27        1.300     1464952509 0.1712301 19.08037      1\n#  28        1.332     1464952511 0.1575612 19.07895      1\n#  29        1.238     1464952513 0.1379985 19.07666      1\n#  30        1.344     1464952514 0.1967692 19.02391      1\n#  31        1.379     1464952516 0.1810977 18.66586      1\n#  32        1.378     1464952518 0.1946998 18.69759      1\n#  33        1.367     1464952520 0.2214879 18.64603      1\n#  34        1.388     1464952521 0.2650294 16.38227      1\n#  35        1.391     1464952523 0.2213947 16.32841      1", 
            "title": "Optimization of objfun1"
        }, 
        {
            "location": "/in_depth_introduction/index.html#optimization-of-objfun2", 
            "text": "Now let us use  mlrMBO  to optimize  objfun2 , which contains one categorical variable.\nAs we have already mentioned before, in case of factor variables only  focussearch  is suitable and kriging cannot be used as a surrogate model.\nIf we would use  mean  as the infill criterion, any kind of model which can handle factors variables is possible (like regression trees, random forests, linear models and many others).  mbo2 = mbo(objfun2, design = design2, learner = surr.rf, control = control2, show.info = FALSE)  mbo2\n#  Recommended parameters:\n#  j=0.462; k=2; method=b\n#  Objective: y = 2.236\n#  \n#  Optimization path\n#  15 + 10 entries in total, displaying last 10 (or less):\n#             j k method        y dob eol error.message exec.time      mean\n#  16 0.4757427 2      b 2.235904   1  NA           NA      0.000 -2.026581\n#  17 0.4849609 2      b 2.235560   2  NA           NA      0.000 -2.082962\n#  18 0.4717123 2      b 2.235995   3  NA           NA      0.000 -2.119596\n#  19 0.4777613 2      b 2.235845   4  NA           NA      0.000 -2.138953\n#  20 0.4674350 2      b 2.236052   5  NA           NA      0.001 -2.152694\n#  21 0.4660672 2      b 2.236061   6  NA           NA      0.000 -2.161759\n#  22 0.4622545 2      b 2.236066   7  NA           NA      0.000 -2.172939\n#  23 0.4673248 2      b 2.236053   8  NA           NA      0.000 -2.174135\n#  24 0.4533910 2      b 2.235950   9  NA           NA      0.000 -2.175038\n#  25 0.4587362 2      b 2.236041  10  NA           NA      0.000 -2.187947\n#     error.model train.time   prop.type propose.time exec.timestamp\n#  16         NA       0.007 infill_mean        0.496     1464952524\n#  17         NA       0.006 infill_mean        0.507     1464952525\n#  18         NA       0.006 infill_mean        0.511     1464952525\n#  19         NA       0.006 infill_mean        0.520     1464952526\n#  20         NA       0.006 infill_mean        0.544     1464952527\n#  21         NA       0.007 infill_mean        0.547     1464952528\n#  22         NA       0.008 infill_mean        0.524     1464952529\n#  23         NA       0.007 infill_mean        0.566     1464952530\n#  24         NA       0.007 infill_mean        0.594     1464952531\n#  25         NA       0.007 infill_mean        0.622     1464952532  If we want to use the expected improvement  ei  or (lower) confidence bound  cb  infill criteria, the  predict.type  attribute of the learner has be set to  se . A list of regression learners which support it can be viewed by:  listLearners(obj =  regr , properties =  se )  We modify the random forest to predict the standard error and optimize  objfun2  by the  ei  infill criterion.  learner_rf = makeLearner( regr.randomForest , predict.type =  se )\ncontrol2$infill.crit =  ei \nmbo2 = mbo(objfun2, design = design2, learner = learner_rf, \n           control = control2, show.info = FALSE)  mbo2\n#  Recommended parameters:\n#  j=0.428; k=2; method=b\n#  Objective: y = 2.235\n#  \n#  Optimization path\n#  15 + 10 entries in total, displaying last 10 (or less):\n#             j k method        y dob eol error.message exec.time\n#  16 0.4087665 2      b 2.232701   1  NA           NA      0.000\n#  17 0.4087998 2      b 2.232705   2  NA           NA      0.000\n#  18 0.4033413 2      b 2.232003   3  NA           NA      0.001\n#  19 0.3865188 2      b 2.229420   4  NA           NA      0.000\n#  20 0.3744102 2      b 2.227171   5  NA           NA      0.000\n#  21 0.3983756 2      b 2.231306   6  NA           NA      0.000\n#  22 0.3475329 2      b 2.221011   7  NA           NA      0.000\n#  23 0.3283410 2      b 2.215630   8  NA           NA      0.000\n#  24 0.3200959 2      b 2.213068   9  NA           NA      0.000\n#  25 0.3247575 2      b 2.214535  10  NA           NA      0.000\n#               ei error.model train.time prop.type propose.time\n#  16 -0.001032499         NA       0.177 infill_ei       33.625\n#  17 -0.003238726         NA       0.168 infill_ei       31.749\n#  18 -0.002051443         NA       0.201 infill_ei       31.214\n#  19 -0.002915200         NA       0.183 infill_ei       31.944\n#  20 -0.002450279         NA       0.173 infill_ei       31.415\n#  21 -0.002619635         NA       0.175 infill_ei       31.749\n#  22 -0.004576781         NA       0.178 infill_ei       32.058\n#  23 -0.005052992         NA       0.180 infill_ei       31.290\n#  24 -0.003741054         NA       0.180 infill_ei       31.704\n#  25 -0.004406909         NA       0.194 infill_ei       32.131\n#     exec.timestamp         se     mean\n#  16     1464952566 0.14104875 1.944803\n#  17     1464952599 0.14306666 2.004142\n#  18     1464952630 0.10511671 2.058761\n#  19     1464952663 0.13876233 2.006720\n#  20     1464952695 0.09833344 2.080229\n#  21     1464952727 0.08121638 2.116259\n#  22     1464952760 0.10829462 2.090093\n#  23     1464952792 0.12091591 2.072615\n#  24     1464952824 0.10710490 2.082343\n#  25     1464952857 0.08455944 2.130153  Finally, if a learner, which does not support the  se  prediction type, should be applied for the optimization with the  ei  infill criterion, it is  possible to create a bagging model with the desired characteristics. For details on how to do it take a look at the  bagging section  in the  mlr  tutorial.", 
            "title": "Optimization of objfun2"
        }, 
        {
            "location": "/parallelization/index.html", 
            "text": "Work in progress", 
            "title": "Parallelization"
        }, 
        {
            "location": "/multi_point_proposal/index.html", 
            "text": "Work in progress!", 
            "title": "Multi point proposal"
        }, 
        {
            "location": "/multi_criteria_optimization/index.html", 
            "text": "Work in progress!", 
            "title": "Multi-criteria optimization"
        }
    ]
}