{
    "docs": [
        {
            "location": "/index.html", 
            "text": "mlrMBO Tutorial\n\n\nmlrMBO\n is a framework for the (sequential) Model Based parameter Optimization.\nThe goal is to optimize numeric or discrete influence parameters\nof a non-linear black box function like an industrial simulator or  a time-consuming algorithm.\n\n\nIn the following we provide an in-depth introduction to \nmlrMBO\n. An introductory example serves as a quickstart guide.\nNote that our focus is on your comprehension of the basic functions and\napplications. For detailed technical information and manual pages, please refer to\nthe package's \nmanual pages\n. They are regularly updated and reflect the documentation of the current packages on CRAN.\n\n\n\n\nQuickstart\n\n\nIn Depth Introduction\n\n\nFurther advanced topics:\n\n\nParallelization\n Make use of multicore CPUs and other distributed computing methods.", 
            "title": "Home"
        }, 
        {
            "location": "/index.html#mlrmbo-tutorial", 
            "text": "mlrMBO  is a framework for the (sequential) Model Based parameter Optimization.\nThe goal is to optimize numeric or discrete influence parameters\nof a non-linear black box function like an industrial simulator or  a time-consuming algorithm.  In the following we provide an in-depth introduction to  mlrMBO . An introductory example serves as a quickstart guide.\nNote that our focus is on your comprehension of the basic functions and\napplications. For detailed technical information and manual pages, please refer to\nthe package's  manual pages . They are regularly updated and reflect the documentation of the current packages on CRAN.   Quickstart  In Depth Introduction  Further advanced topics:  Parallelization  Make use of multicore CPUs and other distributed computing methods.", 
            "title": "mlrMBO Tutorial"
        }, 
        {
            "location": "/quickstart/index.html", 
            "text": "Introductory example\n\n\nInfo:\n this guide gives you an overview of the typical structure of optimization with \nmrMBO\n. For a much more\ndetailed introduction see \nthe next chapter\n.\n\n\nHere we provide a Quickstart example for you to make yourself familiar with \nmlrMBO\n. We aim to optimize the one dimensional Rastrigin function using model-based optimization. Instead of writing this function by hand, we make use of the smoof library, which offers a lot of common single objective optimization functions.\n\n\nlibrary(smoof)\nlibrary(mlr)\nlibrary(mlrMBO)\nlibrary(ParamHelpers)\nobj.fun = makeRastriginFunction(1)\n\n\n\n\nNote:\n Since all this stuff here is under heavy developement it might be neccessary to install the github developement version of the ParamHelpers package via the \ndevtools::install_github\n function.\n\n\nWe decide ourself to use kriging as our surrogate model and to do 10 sequential optimization steps. Furthermore we use Expected Improvement (EI) as the infill criterion, i. e., the criterion which determines which point(s) of the objective function should be evaluated in further iterations (keep in mind, that using EI as the infill criterion needs the learner to support standard error estimation). \n\n\nlearner = makeLearner(\nregr.km\n, predict.type = \nse\n, covtype = \nmatern3_2\n)\ncontrol = makeMBOControl()\ncontrol = setMBOControlTermination(control, iters = 10)\ncontrol = setMBOControlInfill(control, crit = \nei\n)\ncontrol = setMBOControlTermination(control, iters = 10)\ndesign = generateDesign(n = 10, par.set = smoof::getParamSet(obj.fun))\n\n\n\n\nFinally we start the optimization process and print the result object.\n\n\nresult = mbo(obj.fun, design = design, learner = learner, control = control, show.info = TRUE)\n#\n Computing y column(s) for design. Not provided.\n#\n [mbo] 0: x=4.17 : y = 22.8 : 0.0 secs : initdesign\n#\n [mbo] 0: x=2.74 : y = 18.4 : 0.0 secs : initdesign\n#\n [mbo] 0: x=0.272 : y = 11.5 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-1.65 : y = 18.4 : 0.0 secs : initdesign\n#\n [mbo] 0: x=2.03 : y = 4.32 : 0.0 secs : initdesign\n#\n [mbo] 0: x=3.11 : y = 12.1 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-2.44 : y = 25.3 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-3.89 : y = 17.4 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-0.769 : y = 9.37 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-4.37 : y = 35.9 : 0.0 secs : initdesign\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -42.16228 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       42.162  |proj g|=       1.9768\n#\n At iterate     1  f =       35.839  |proj g|=       0.31573\n#\n At iterate     2  f =        35.68  |proj g|=             0\n#\n \n#\n iterations 2\n#\n function evaluations 3\n#\n segments explored during Cauchy searches 2\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 35.6801\n#\n \n#\n F = 35.6801\n#\n final  value 35.680121 \n#\n converged\n#\n [mbo] 1: x=-2.05 : y = 4.7 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -50.85536 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       50.855  |proj g|=       1.8606\n#\n At iterate     1  f =       39.744  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 39.7438\n#\n \n#\n F = 39.7438\n#\n final  value 39.743841 \n#\n converged\n#\n [mbo] 2: x=3.62 : y = 30.3 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -46.05445 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       46.054  |proj g|=      0.45522\n#\n At iterate     1  f =       43.926  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 43.9265\n#\n \n#\n F = 43.9265\n#\n final  value 43.926485 \n#\n converged\n#\n [mbo] 3: x=-3.55 : y = 32.2 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -49.18026 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=        49.18  |proj g|=      0.30591\n#\n At iterate     1  f =       48.175  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 48.1752\n#\n \n#\n F = 48.1752\n#\n final  value 48.175161 \n#\n converged\n#\n [mbo] 4: x=3.72 : y = 25.6 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -52.05361 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       52.054  |proj g|=      0.30731\n#\n At iterate     1  f =       51.605  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 51.6047\n#\n \n#\n F = 51.6047\n#\n final  value 51.604695 \n#\n converged\n#\n [mbo] 5: x=-3.79 : y = 22 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -54.14426 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       54.144  |proj g|=       5.6298\n#\n At iterate     1  f =       54.011  |proj g|=       0.22682\n#\n At iterate     2  f =       53.963  |proj g|=        0.1198\n#\n At iterate     3  f =       53.963  |proj g|=     0.0074116\n#\n At iterate     4  f =       53.963  |proj g|=     1.392e-05\n#\n At iterate     5  f =       53.963  |proj g|=    1.5912e-09\n#\n \n#\n iterations 5\n#\n function evaluations 8\n#\n segments explored during Cauchy searches 5\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 1.59118e-09\n#\n final function value 53.9628\n#\n \n#\n F = 53.9628\n#\n final  value 53.962788 \n#\n converged\n#\n [mbo] 6: x=1.94 : y = 4.5 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -57.42694 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       57.427  |proj g|=      0.37626\n#\n At iterate     1  f =       56.944  |proj g|=       0.31386\n#\n At iterate     2  f =       56.668  |proj g|=       0.24689\n#\n At iterate     3  f =       56.657  |proj g|=       0.52416\n#\n At iterate     4  f =       56.656  |proj g|=      0.025438\n#\n At iterate     5  f =       56.656  |proj g|=    0.00038393\n#\n At iterate     6  f =       56.656  |proj g|=    2.9003e-07\n#\n \n#\n iterations 6\n#\n function evaluations 9\n#\n segments explored during Cauchy searches 6\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 2.90031e-07\n#\n final function value 56.6555\n#\n \n#\n F = 56.6555\n#\n final  value 56.655547 \n#\n converged\n#\n [mbo] 7: x=-1.96 : y = 4.13 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -68.84067 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       68.841  |proj g|=       1.4873\n#\n At iterate     1  f =       63.315  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 63.3155\n#\n \n#\n F = 63.3155\n#\n final  value 63.315486 \n#\n converged\n#\n [mbo] 8: x=1.13 : y = 4.36 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -64.75254 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       64.753  |proj g|=      0.52075\n#\n At iterate     1  f =       63.962  |proj g|=       0.43422\n#\n At iterate     2  f =       63.285  |proj g|=       0.30913\n#\n At iterate     3  f =       63.269  |proj g|=        1.1478\n#\n At iterate     4  f =       63.261  |proj g|=      0.074181\n#\n At iterate     5  f =       63.261  |proj g|=     0.0026487\n#\n At iterate     6  f =       63.261  |proj g|=    6.4719e-06\n#\n \n#\n iterations 6\n#\n function evaluations 9\n#\n segments explored during Cauchy searches 6\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 6.47193e-06\n#\n final function value 63.2613\n#\n \n#\n F = 63.2613\n#\n final  value 63.261339 \n#\n converged\n#\n [mbo] 9: x=1.32 : y = 16 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -76.513 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       76.513  |proj g|=       1.3051\n#\n At iterate     1  f =       70.586  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 70.586\n#\n \n#\n F = 70.586\n#\n final  value 70.586034 \n#\n converged\n#\n [mbo] 10: x=1.41 : y = 20.4 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  17.08579 \n#\n   - best initial criterion value(s) :  -85.50927 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       85.509  |proj g|=       1.4973\n#\n At iterate     1  f =       82.144  |proj g|=        2.0383\n#\n ys=-2.885e+00  -gs= 2.242e+00, BFGS update SKIPPED\n#\n At iterate     2  f =       73.855  |proj g|=             0\n#\n \n#\n iterations 2\n#\n function evaluations 3\n#\n segments explored during Cauchy searches 2\n#\n BFGS updates skipped 1\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 73.855\n#\n \n#\n F = 73.855\n#\n final  value 73.854960 \n#\n converged\nprint(result)\n#\n Recommended parameters:\n#\n x=-1.96\n#\n Objective: y = 4.134\n#\n \n#\n Optimization path\n#\n 10 + 10 entries in total, displaying last 10 (or less):\n#\n            x         y dob eol error.message exec.time         ei\n#\n 11 -2.050284  4.698625   1  NA          \nNA\n     0.000 -0.2288446\n#\n 12  3.623820 30.255372   2  NA          \nNA\n     0.000 -0.3726289\n#\n 13 -3.546339 32.155650   3  NA          \nNA\n     0.000 -0.3422541\n#\n 14  3.722633 25.569052   4  NA          \nNA\n     0.001 -0.3188126\n#\n 15 -3.787732 21.998262   5  NA          \nNA\n     0.000 -0.2604270\n#\n 16  1.938323  4.498631   6  NA          \nNA\n     0.000 -1.1183878\n#\n 17 -1.961884  4.134397   7  NA          \nNA\n     0.000 -0.9190494\n#\n 18  1.128445  4.357019   8  NA          \nNA\n     0.000 -0.4194018\n#\n 19  1.319269 15.956642   9  NA          \nNA\n     0.000 -1.4304011\n#\n 20  1.409904 20.427879  10  NA          \nNA\n     0.000 -0.4781482\n#\n    error.model train.time prop.type propose.time exec.timestamp        se\n#\n 11        \nNA\n      0.018 infill_ei        0.231     1462797170  8.576909\n#\n 12        \nNA\n      0.018 infill_ei        0.358     1462797171  8.972263\n#\n 13        \nNA\n      0.019 infill_ei        0.344     1462797172  9.408390\n#\n 14        \nNA\n      0.018 infill_ei        0.232     1462797172  9.843812\n#\n 15        \nNA\n      0.016 infill_ei        0.235     1462797173  9.651479\n#\n 16        \nNA\n      0.021 infill_ei        0.259     1462797173  5.725409\n#\n 17        \nNA\n      0.020 infill_ei        0.261     1462797174  4.622039\n#\n 18        \nNA\n      0.019 infill_ei        0.377     1462797174 10.029210\n#\n 19        \nNA\n      0.023 infill_ei        0.283     1462797175  7.264680\n#\n 20        \nNA\n      0.018 infill_ei        0.265     1462797175  9.935517\n#\n         mean\n#\n 11 17.538842\n#\n 12 16.371550\n#\n 13 17.528535\n#\n 14 18.653697\n#\n 15 19.147651\n#\n 16  7.231362\n#\n 17  6.618227\n#\n 18 17.570495\n#\n 19  7.787929\n#\n 20 16.790109", 
            "title": "Quickstart"
        }, 
        {
            "location": "/quickstart/index.html#introductory-example", 
            "text": "Info:  this guide gives you an overview of the typical structure of optimization with  mrMBO . For a much more\ndetailed introduction see  the next chapter .  Here we provide a Quickstart example for you to make yourself familiar with  mlrMBO . We aim to optimize the one dimensional Rastrigin function using model-based optimization. Instead of writing this function by hand, we make use of the smoof library, which offers a lot of common single objective optimization functions.  library(smoof)\nlibrary(mlr)\nlibrary(mlrMBO)\nlibrary(ParamHelpers)\nobj.fun = makeRastriginFunction(1)  Note:  Since all this stuff here is under heavy developement it might be neccessary to install the github developement version of the ParamHelpers package via the  devtools::install_github  function.  We decide ourself to use kriging as our surrogate model and to do 10 sequential optimization steps. Furthermore we use Expected Improvement (EI) as the infill criterion, i. e., the criterion which determines which point(s) of the objective function should be evaluated in further iterations (keep in mind, that using EI as the infill criterion needs the learner to support standard error estimation).   learner = makeLearner( regr.km , predict.type =  se , covtype =  matern3_2 )\ncontrol = makeMBOControl()\ncontrol = setMBOControlTermination(control, iters = 10)\ncontrol = setMBOControlInfill(control, crit =  ei )\ncontrol = setMBOControlTermination(control, iters = 10)\ndesign = generateDesign(n = 10, par.set = smoof::getParamSet(obj.fun))  Finally we start the optimization process and print the result object.  result = mbo(obj.fun, design = design, learner = learner, control = control, show.info = TRUE)\n#  Computing y column(s) for design. Not provided.\n#  [mbo] 0: x=4.17 : y = 22.8 : 0.0 secs : initdesign\n#  [mbo] 0: x=2.74 : y = 18.4 : 0.0 secs : initdesign\n#  [mbo] 0: x=0.272 : y = 11.5 : 0.0 secs : initdesign\n#  [mbo] 0: x=-1.65 : y = 18.4 : 0.0 secs : initdesign\n#  [mbo] 0: x=2.03 : y = 4.32 : 0.0 secs : initdesign\n#  [mbo] 0: x=3.11 : y = 12.1 : 0.0 secs : initdesign\n#  [mbo] 0: x=-2.44 : y = 25.3 : 0.0 secs : initdesign\n#  [mbo] 0: x=-3.89 : y = 17.4 : 0.0 secs : initdesign\n#  [mbo] 0: x=-0.769 : y = 9.37 : 0.0 secs : initdesign\n#  [mbo] 0: x=-4.37 : y = 35.9 : 0.0 secs : initdesign\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -42.16228 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       42.162  |proj g|=       1.9768\n#  At iterate     1  f =       35.839  |proj g|=       0.31573\n#  At iterate     2  f =        35.68  |proj g|=             0\n#  \n#  iterations 2\n#  function evaluations 3\n#  segments explored during Cauchy searches 2\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 35.6801\n#  \n#  F = 35.6801\n#  final  value 35.680121 \n#  converged\n#  [mbo] 1: x=-2.05 : y = 4.7 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -50.85536 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       50.855  |proj g|=       1.8606\n#  At iterate     1  f =       39.744  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 39.7438\n#  \n#  F = 39.7438\n#  final  value 39.743841 \n#  converged\n#  [mbo] 2: x=3.62 : y = 30.3 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -46.05445 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       46.054  |proj g|=      0.45522\n#  At iterate     1  f =       43.926  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 43.9265\n#  \n#  F = 43.9265\n#  final  value 43.926485 \n#  converged\n#  [mbo] 3: x=-3.55 : y = 32.2 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -49.18026 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=        49.18  |proj g|=      0.30591\n#  At iterate     1  f =       48.175  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 48.1752\n#  \n#  F = 48.1752\n#  final  value 48.175161 \n#  converged\n#  [mbo] 4: x=3.72 : y = 25.6 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -52.05361 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       52.054  |proj g|=      0.30731\n#  At iterate     1  f =       51.605  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 51.6047\n#  \n#  F = 51.6047\n#  final  value 51.604695 \n#  converged\n#  [mbo] 5: x=-3.79 : y = 22 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -54.14426 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       54.144  |proj g|=       5.6298\n#  At iterate     1  f =       54.011  |proj g|=       0.22682\n#  At iterate     2  f =       53.963  |proj g|=        0.1198\n#  At iterate     3  f =       53.963  |proj g|=     0.0074116\n#  At iterate     4  f =       53.963  |proj g|=     1.392e-05\n#  At iterate     5  f =       53.963  |proj g|=    1.5912e-09\n#  \n#  iterations 5\n#  function evaluations 8\n#  segments explored during Cauchy searches 5\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 0\n#  norm of the final projected gradient 1.59118e-09\n#  final function value 53.9628\n#  \n#  F = 53.9628\n#  final  value 53.962788 \n#  converged\n#  [mbo] 6: x=1.94 : y = 4.5 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -57.42694 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       57.427  |proj g|=      0.37626\n#  At iterate     1  f =       56.944  |proj g|=       0.31386\n#  At iterate     2  f =       56.668  |proj g|=       0.24689\n#  At iterate     3  f =       56.657  |proj g|=       0.52416\n#  At iterate     4  f =       56.656  |proj g|=      0.025438\n#  At iterate     5  f =       56.656  |proj g|=    0.00038393\n#  At iterate     6  f =       56.656  |proj g|=    2.9003e-07\n#  \n#  iterations 6\n#  function evaluations 9\n#  segments explored during Cauchy searches 6\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 0\n#  norm of the final projected gradient 2.90031e-07\n#  final function value 56.6555\n#  \n#  F = 56.6555\n#  final  value 56.655547 \n#  converged\n#  [mbo] 7: x=-1.96 : y = 4.13 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -68.84067 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       68.841  |proj g|=       1.4873\n#  At iterate     1  f =       63.315  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 63.3155\n#  \n#  F = 63.3155\n#  final  value 63.315486 \n#  converged\n#  [mbo] 8: x=1.13 : y = 4.36 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -64.75254 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       64.753  |proj g|=      0.52075\n#  At iterate     1  f =       63.962  |proj g|=       0.43422\n#  At iterate     2  f =       63.285  |proj g|=       0.30913\n#  At iterate     3  f =       63.269  |proj g|=        1.1478\n#  At iterate     4  f =       63.261  |proj g|=      0.074181\n#  At iterate     5  f =       63.261  |proj g|=     0.0026487\n#  At iterate     6  f =       63.261  |proj g|=    6.4719e-06\n#  \n#  iterations 6\n#  function evaluations 9\n#  segments explored during Cauchy searches 6\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 0\n#  norm of the final projected gradient 6.47193e-06\n#  final function value 63.2613\n#  \n#  F = 63.2613\n#  final  value 63.261339 \n#  converged\n#  [mbo] 9: x=1.32 : y = 16 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -76.513 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       76.513  |proj g|=       1.3051\n#  At iterate     1  f =       70.586  |proj g|=             0\n#  \n#  iterations 1\n#  function evaluations 2\n#  segments explored during Cauchy searches 1\n#  BFGS updates skipped 0\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 70.586\n#  \n#  F = 70.586\n#  final  value 70.586034 \n#  converged\n#  [mbo] 10: x=1.41 : y = 20.4 : 0.0 secs : infill_ei\n#  \n#  optimisation start\n#  ------------------\n#  * estimation method   : MLE \n#  * optimisation method : BFGS \n#  * analytical gradient : used\n#  * trend model : ~1\n#  * covariance model : \n#    - type :  matern3_2 \n#    - nugget : NO\n#    - parameters lower bounds :  1e-10 \n#    - parameters upper bounds :  17.08579 \n#    - best initial criterion value(s) :  -85.50927 \n#  \n#  N = 1, M = 5 machine precision = 2.22045e-16\n#  At X0, 0 variables are exactly at the bounds\n#  At iterate     0  f=       85.509  |proj g|=       1.4973\n#  At iterate     1  f =       82.144  |proj g|=        2.0383\n#  ys=-2.885e+00  -gs= 2.242e+00, BFGS update SKIPPED\n#  At iterate     2  f =       73.855  |proj g|=             0\n#  \n#  iterations 2\n#  function evaluations 3\n#  segments explored during Cauchy searches 2\n#  BFGS updates skipped 1\n#  active bounds at final generalized Cauchy point 1\n#  norm of the final projected gradient 0\n#  final function value 73.855\n#  \n#  F = 73.855\n#  final  value 73.854960 \n#  converged\nprint(result)\n#  Recommended parameters:\n#  x=-1.96\n#  Objective: y = 4.134\n#  \n#  Optimization path\n#  10 + 10 entries in total, displaying last 10 (or less):\n#             x         y dob eol error.message exec.time         ei\n#  11 -2.050284  4.698625   1  NA           NA      0.000 -0.2288446\n#  12  3.623820 30.255372   2  NA           NA      0.000 -0.3726289\n#  13 -3.546339 32.155650   3  NA           NA      0.000 -0.3422541\n#  14  3.722633 25.569052   4  NA           NA      0.001 -0.3188126\n#  15 -3.787732 21.998262   5  NA           NA      0.000 -0.2604270\n#  16  1.938323  4.498631   6  NA           NA      0.000 -1.1183878\n#  17 -1.961884  4.134397   7  NA           NA      0.000 -0.9190494\n#  18  1.128445  4.357019   8  NA           NA      0.000 -0.4194018\n#  19  1.319269 15.956642   9  NA           NA      0.000 -1.4304011\n#  20  1.409904 20.427879  10  NA           NA      0.000 -0.4781482\n#     error.model train.time prop.type propose.time exec.timestamp        se\n#  11         NA       0.018 infill_ei        0.231     1462797170  8.576909\n#  12         NA       0.018 infill_ei        0.358     1462797171  8.972263\n#  13         NA       0.019 infill_ei        0.344     1462797172  9.408390\n#  14         NA       0.018 infill_ei        0.232     1462797172  9.843812\n#  15         NA       0.016 infill_ei        0.235     1462797173  9.651479\n#  16         NA       0.021 infill_ei        0.259     1462797173  5.725409\n#  17         NA       0.020 infill_ei        0.261     1462797174  4.622039\n#  18         NA       0.019 infill_ei        0.377     1462797174 10.029210\n#  19         NA       0.023 infill_ei        0.283     1462797175  7.264680\n#  20         NA       0.018 infill_ei        0.265     1462797175  9.935517\n#          mean\n#  11 17.538842\n#  12 16.371550\n#  13 17.528535\n#  14 18.653697\n#  15 19.147651\n#  16  7.231362\n#  17  6.618227\n#  18 17.570495\n#  19  7.787929\n#  20 16.790109", 
            "title": "Introductory example"
        }, 
        {
            "location": "/in_depth_introduction/index.html", 
            "text": "The first step of MBO requires an initial set of evaluation points which is then evaluated by the black box function.\nThe basic procedure of MBO is an iterating loop of the following steps:\nFirstly, a user defined surrogate model is fitted on the evaluated points, secondly, a new evaluation point is proposed by an infill criterion and lastly, its performance is evaluated.\nThe result of this sequential procedure is the optimization path containing the best parameter setting and the fitted surrogate model.\n\n\nOnce again, take a look at the already introduced basic example of the optimization of the one dimensional Rastrigin function.\n\n\nlibrary(mlrMBO)\n#\n Loading required package: lhs\n#\n Loading required package: smoof\n#\n Loading required package: checkmate\n#\n \n#\n Attaching package: 'smoof'\n#\n The following object is masked from 'package:mlr':\n#\n \n#\n     getParamSet\n#\n Warning: replacing previous import by 'smoof::getParamSet' when loading\n#\n 'mlrMBO'\n#\n \n#\n Attaching package: 'mlrMBO'\n#\n The following object is masked from 'package:ParamHelpers':\n#\n \n#\n     plotEAF\nobj.fun = makeRastriginFunction(1)\n\nlearner = makeLearner(\nregr.km\n, predict.type = \nse\n, covtype = \nmatern3_2\n)\ncontrol = makeMBOControl()\ncontrol = setMBOControlTermination(control, iters = 5)\ncontrol = setMBOControlInfill(control, crit = \nei\n)\n\nresult = mbo(obj.fun, learner = learner, control = control, show.info = TRUE)\n#\n Computing y column(s) for design. Not provided.\n#\n [mbo] 0: x=3.29 : y = 23.5 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-3.22 : y = 18.5 : 0.0 secs : initdesign\n#\n [mbo] 0: x=-0.777 : y = 8.9 : 0.0 secs : initdesign\n#\n [mbo] 0: x=1.93 : y = 4.59 : 0.0 secs : initdesign\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  13.02456 \n#\n   - best initial criterion value(s) :  -14.00775 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       14.008  |proj g|=      0.72708\n#\n At iterate     1  f =       13.729  |proj g|=             0\n#\n \n#\n iterations 1\n#\n function evaluations 2\n#\n segments explored during Cauchy searches 1\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 1\n#\n norm of the final projected gradient 0\n#\n final function value 13.7295\n#\n \n#\n F = 13.7295\n#\n final  value 13.729484 \n#\n converged\n#\n [mbo] 1: x=-4.72 : y = 33.9 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  16.03308 \n#\n   - best initial criterion value(s) :  -18.8355 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       18.835  |proj g|=     0.075704\n#\n At iterate     1  f =       18.832  |proj g|=      0.019382\n#\n At iterate     2  f =       18.832  |proj g|=     0.0091959\n#\n At iterate     3  f =       18.832  |proj g|=     0.0038259\n#\n At iterate     4  f =       18.832  |proj g|=     0.0017401\n#\n At iterate     5  f =       18.832  |proj g|=    0.00078512\n#\n At iterate     6  f =       18.832  |proj g|=    0.00036173\n#\n At iterate     7  f =       18.832  |proj g|=    0.00016757\n#\n At iterate     8  f =       18.832  |proj g|=    7.8277e-05\n#\n At iterate     9  f =       18.832  |proj g|=    3.6757e-05\n#\n At iterate    10  f =       18.832  |proj g|=    1.7346e-05\n#\n At iterate    11  f =       18.832  |proj g|=    8.2183e-06\n#\n At iterate    12  f =       18.832  |proj g|=    3.9073e-06\n#\n \n#\n iterations 12\n#\n function evaluations 13\n#\n segments explored during Cauchy searches 12\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 3.90729e-06\n#\n final function value 18.8316\n#\n \n#\n F = 18.8316\n#\n final  value 18.831551 \n#\n converged\n#\n [mbo] 2: x=1.87 : y = 6.7 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  16.03308 \n#\n   - best initial criterion value(s) :  -21.02962 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=        21.03  |proj g|=      0.61913\n#\n At iterate     1  f =       20.983  |proj g|=       0.57786\n#\n At iterate     2  f =       20.921  |proj g|=       0.27495\n#\n At iterate     3  f =       20.917  |proj g|=       0.04167\n#\n At iterate     4  f =       20.917  |proj g|=     0.0014829\n#\n At iterate     5  f =       20.917  |proj g|=    8.3302e-06\n#\n At iterate     6  f =       20.917  |proj g|=    1.6546e-09\n#\n \n#\n iterations 6\n#\n function evaluations 8\n#\n segments explored during Cauchy searches 6\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 1.65464e-09\n#\n final function value 20.9172\n#\n \n#\n F = 20.9172\n#\n final  value 20.917239 \n#\n converged\n#\n [mbo] 3: x=2.14 : y = 8.09 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  16.03308 \n#\n   - best initial criterion value(s) :  -24.67325 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       24.673  |proj g|=      0.75882\n#\n At iterate     1  f =        24.03  |proj g|=       0.61425\n#\n At iterate     2  f =       23.612  |proj g|=       0.43672\n#\n At iterate     3  f =       23.607  |proj g|=       0.22142\n#\n At iterate     4  f =       23.607  |proj g|=     0.0062194\n#\n At iterate     5  f =       23.607  |proj g|=    6.9388e-05\n#\n At iterate     6  f =       23.607  |proj g|=     2.234e-08\n#\n \n#\n iterations 6\n#\n function evaluations 9\n#\n segments explored during Cauchy searches 6\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 2.23399e-08\n#\n final function value 23.6066\n#\n \n#\n F = 23.6066\n#\n final  value 23.606588 \n#\n converged\n#\n [mbo] 4: x=-0.399 : y = 18.2 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  16.03308 \n#\n   - best initial criterion value(s) :  -27.64944 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       27.649  |proj g|=       8.5093\n#\n At iterate     1  f =       27.419  |proj g|=       0.63398\n#\n At iterate     2  f =       26.838  |proj g|=       0.47073\n#\n At iterate     3  f =       26.803  |proj g|=        1.7961\n#\n At iterate     4  f =       26.756  |proj g|=       0.19972\n#\n At iterate     5  f =       26.755  |proj g|=      0.014908\n#\n At iterate     6  f =       26.755  |proj g|=    0.00015116\n#\n At iterate     7  f =       26.755  |proj g|=    1.1164e-07\n#\n \n#\n iterations 7\n#\n function evaluations 10\n#\n segments explored during Cauchy searches 7\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 1.11641e-07\n#\n final function value 26.7553\n#\n \n#\n F = 26.7553\n#\n final  value 26.755343 \n#\n converged\n#\n [mbo] 5: x=1.99 : y = 3.98 : 0.0 secs : infill_ei\n#\n \n#\n optimisation start\n#\n ------------------\n#\n * estimation method   : MLE \n#\n * optimisation method : BFGS \n#\n * analytical gradient : used\n#\n * trend model : ~1\n#\n * covariance model : \n#\n   - type :  matern3_2 \n#\n   - nugget : NO\n#\n   - parameters lower bounds :  1e-10 \n#\n   - parameters upper bounds :  16.03308 \n#\n   - best initial criterion value(s) :  -27.94255 \n#\n \n#\n N = 1, M = 5 machine precision = 2.22045e-16\n#\n At X0, 0 variables are exactly at the bounds\n#\n At iterate     0  f=       27.943  |proj g|=      0.59855\n#\n At iterate     1  f =       27.852  |proj g|=       0.56757\n#\n At iterate     2  f =       27.741  |proj g|=        1.5549\n#\n At iterate     3  f =       27.709  |proj g|=       0.32923\n#\n At iterate     4  f =       27.707  |proj g|=      0.029886\n#\n At iterate     5  f =       27.707  |proj g|=    0.00068044\n#\n At iterate     6  f =       27.707  |proj g|=    1.3519e-06\n#\n \n#\n iterations 6\n#\n function evaluations 8\n#\n segments explored during Cauchy searches 6\n#\n BFGS updates skipped 0\n#\n active bounds at final generalized Cauchy point 0\n#\n norm of the final projected gradient 1.35189e-06\n#\n final function value 27.7075\n#\n \n#\n F = 27.7075\n#\n final  value 27.707455 \n#\n converged\n\n\n\n\nFrom this example we can easily recognize some \nmlrMBO\n essentials like parameters, learners and the control object.\n\n\nBasically the following steps are needed to start a surrogate-based optimization with our package. \nEach step ends with an R object, which is then passed to \nmbo()\n, i. e., to the working horse of mlrMBO.\n\n\n\n\ndefine the objective function and its parameters by using the package \nsmoof\n\n\n(optionally generate an initial design)\n\n\ndefine a learner, i. e., the surrogate model\n\n\nset up a MBO control object, which offers a load of options\n\n\nfinally start the optimization\n\n\n\n\nThis web page will provide you with an in-depth introduction on how to set the \nmbo()\n parameters depending on the desired kind of optimization.\n\n\nObjective Function\n\n\nThe first argument of \nmbo()\n is the name of the objective function created with the package \nsmoof\n. \nThe first argument of this objective function has to be a list of values.\nThe function has to return a single numerical value. \nWe demonstrate in this tutorial the optimization of two simple functions: The 5 dimensional \nackley function\n (\nobjfun1\n) and a self-constructed sine und cosine combination (\nobjfun2\n), where we do not minimize but maximize the function. \n\nobjfun1\n depends on 5 numeric parameters, while \nobjfun2\n assumes 2 numeric and 1 discrete parameters.\n\n\nThe 5 dimensional \nackley function\n can be get by using the appropriate function of the \nsmoof\n package\n\n\nobjfun1 = makeAckleyFunction(5) \n\n\n\n\nThe self-constructed function is built with \nmakeSingleObjetiveFunction\n. \nIt has the attribute \npar.set\n that has to be a ParamSet object from the \nParamHelpers\n package, which provides information about the parameters of the objective function and their constraints for optimization.\nWe assume \nj\n from interval [0,1] and \nk\n from interval [1,2]. Parameter \nmethod\n can be either \n\"a\"\n or \n\"b\"\n.\nThe type of optimization, in this case maximization, is also defined in \nmakeSingleObjetiveFunction\n with \nminimize = TRUE\n as default.\n\n\nfoo = function(x) {\n  j = x[[1]]\n  k = x[[2]]\n  method = x[[3]]\n  perf = ifelse(method == \na\n, k * sin(j) + cos(j),\n               sin(j) + k * cos(j))\n  return(perf)\n}\nobjfun2 = makeSingleObjectiveFunction(\n  name = \nexample\n,\n  fn = foo,\n  par.set = makeParamSet(\n    makeNumericParam(\nj\n, lower = 0,upper = 1),\n    makeIntegerParam(\nk\n, lower = 1, upper = 2),\n    makeDiscreteParam(\nmethod\n, values = c(\na\n, \nb\n))\n  ),\n  has.simple.signature = FALSE,\n  minimize = FALSE\n)\n\n\n\n\nInitial Design\n\n\nThe second argument of the \nmbo()\n function - \ndesign\n - is the initial design with default setting \nNULL\n.\n\n\nIt is recommendable to use the \ngenerateDesign\n function from \nParamHelpers\n package to create it.\nHowever, if special designs are desired (e.g., orthogonal designs), its interface has to be the same as the interface of the \ngenerateDesign\n objects. \nParticular attention has to be paid to the setting of the \ntrafo\n attribute.\n\n\ninit.points1 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun1)))\n# NOTE:\n# init.points1 = 5 * getParamLengths(getParamSet(objfun1))\n# is not working, because mlr::getParamSet() is used by default.\ninit.fun1 = randomLHS\nset.seed(1)\ndesign1 = generateDesign(n = init.points1, par.set = smoof::getParamSet(objfun1), fun = init.fun1, trafo = FALSE)\n\ninit.points2 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun2)))\ninit.fun2 = maximinLHS\ninit.args2 = list(k = 3, dup = 4)\ndesign2 = generateDesign(n = init.points2, par.set = smoof::getParamSet(objfun2), fun.args = init.args2, fun = init.fun2, trafo = FALSE)\n\n\n\n\nSurrogate Model\n\n\nAttribute \nlearner\n of the \nmbo()\n function allows us to choose an appropriate surrogate model for the parameter optimization.\nIt can be easily done using the \nmakeLearner\n function from \nmlr\n package.\nList of implemented learners can be seen using the ?learners command or on \nhttp://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/\n\n\n\nThe choice of the surrogate model depends on the parameter set of the objective function.\nWhile kriging models are advisable for the numeric parameters, random forest models can be used if at least one parameter is factorial.\nIn our example we consider these two surrogate models:\n\nkriging\n for optimizing of \nobjfun1\n  and \nrandom forest\n for \nobjfun2\n.\n\n\nlearner_km = makeLearner(\nregr.km\n, predict.type = \nse\n, covtype = \nmatern3_2\n)\nlearner_rf = makeLearner(\nregr.randomForest\n)\n\n\n\n\nHowever, in some cases it is necessary to modify the learners (e.g., in order to get the standard error prediction for design points).\nThis will be discussed und illustrated in the section \"Experiments and Output\".\n\n\nMBOControl\n\n\nThe \nMBOControl\n object can be created with \nmakeMBOControl\n and some attributes can be set when creating it.\nBut there are also further functions to define or change the settings of the object:  \n\n\nsetMBOControlInfill\n\n\nAttribute \ncrit\n\n\nOne of the most important issues is to define how the next design points in the sequential loops have to be chosen. \nFirstly, we have to choose the infill criterion using the  \ncrit\n attribute of \nsetMBOControlInfill\n.\nAt the moment five possibilities are implemented:\n\n \nmean\n: mean response of the surrogate model,\n\n \nei\n: expected improvement of the surrogate model,\n\n \naei\n: augmented expected improvement, which is especially useful for the noisy functions,\n\n \neqi\n: expected quantile improvement\n* \ncb\n: confidence bound which is the additive combination of mean response and mean standard error estimation of the surrogate model (response - lambda * standard.error). \nThe default value of lambda is 1, but it can easily be changed by\nthe \ncrit.cb.lambda\n attribute.\n\n\nAttribute \nopt\n\n\nThe attribute \nopt\n sets how the next point to evaluate should be proposed given an infill criterion. \nThe possibilities are \nfocussearch\n, \ncmaes\n, \nea\n and \nnsga2\n.\n\n\nIf \nfocussearch\n is chosen, the common procedure is as follows: \nin the first step an lhs design is sampled in the parameter space (by \nrandomLHS\n function) and the design point with the best  prediction of the infill criterion is determined. \nUser can set the size of this design by the \nopt.focussearch.points\n attribute (default value is 10000). \nIn the second step the parameter space is shrunk around the best design point in a certain way which should not be discussed in detail here. \nFirst and second steps are repeated iteratively \nopt.focussearch.maxit\n times (default is 5) while the best seen value of the infill criterion is passed back.\n\n\nIf \nopt\n is \ncmaes\n, the point, which optimizes the\ninfill criterion, is chosen via \ncma_es\n function of  \ncmaes\n package. Control argument for \ncmaes\n optimizer can be provided in\nthe \nopt.cmaes.control\n attribute (default is empty list).\n\n\nIf \nopt\n is \nea\n a simple (mu+1)-evolutionary optimization algorithm is used to optimize the infill criterion.\nThe population size mu can be set by the \nopt.ea.mu\n attribute (default value is 10).\n(mu+1) means that in each population only one child is generated using crossover und mutation operators (from \nemao\n package).\n\n\nThe parameters \neta\n and \np\n of the latter two operators can be adjusted via the attributes \nopt.ea.sbx.eta\n, \nopt.ea.sbx.p\n,\n\nopt.ea.pm.eta\n and \nopt.ea.pm.p\n.\nThe default number of EA iterations is 500 and can be changed by \nopt.ea.maxit\n attribute.\n\n\nFor multi objective optimization, \nopt\n should be set to \nnsga2\n. \nThis is needed for mspot.\n\n\n\nAs all four infill optimization strategies do not guarantee  to find the global optimum, users can set the number of restarts by the \nopt.restarts\n attribute (default value is 1).\nAfter conducting the desired number of restarts the design point with the best infill criterion value is passed back to the MBO loop.\n\n\nPlease note that just \nfocussearch\n optimizer is suitable for the case of factor parameters in the parameter set!\n\n\nFurther functions\n\n\nThe number of sequential steps (iterations) can be set via the attribute \niters\n of the function \nsetMBOControlTermination\n (default setting is 10).\n\n\nThere are also many other functions and attributes which user can set in a desired way...\n\n\n\n\nThe list of all attributes is provided in the software documentation.\n\n\n\ncontrol1 = makeMBOControl()\ncontrol1 = setMBOControlInfill(\n  control = control1,\n  crit = \nei\n,\n  opt = \ncmaes\n\n)\ncontrol1 = setMBOControlTermination(\n  control = control1,\n  iters = 10\n)\n\ncontrol2 = makeMBOControl()\ncontrol2 = setMBOControlInfill(\n  control = control2,\n  crit = \nmean\n,\n  opt = \nfocussearch\n\n)\ncontrol2 = setMBOControlTermination(\n  control = control2,\n  iters = 10\n)\n\n\n\n\nExperiments and Output\n\n\nNow we will apply the mbo() function to optimize the two objective functions.\n\n\nOptimization of \nobjfun1\n\n\n\n\n\nmbo1 = mbo(objfun1, design = design1, learner = learner_km, control = control1, show.info = TRUE)\nmbo1\ngetOptPathY(mbo1$opt.path, \ny\n) # get all y values\n\n\n\n\n\nThe output of mbo function is a structure of several variables. The most important are:\n\n\n\n\nx: the best point of the parameter space\n\n\ny: the associated best value of the objective function\n\n\nopt.path: optimization path. See \nParamHelpers\n for further information.\n\n\nmodels: If no other setting were provided in the \nMBOControl\n object, the last estimated surrogate is given here.\n\n\n...\n\n\n\n\n\n\n\nWe can also change some attributes of the \nMBOControl\n object and run mbo() function again\n\n\ncontrol1$infill.crit = \nmean\n\ncontrol1$infill.opt = \nfocussearch\n\nmbo1 = mbo(objfun1, design = design1, learner = learner_km, control = control1, show.info = FALSE)\nmbo1$y\n\n\n\n\nOptimization of \nobjfun2\n\n\nLet us apply \nmlrMBO\n package to optimize object2 function, which contains one factor variable.\nWe have already mentioned before that in this case just the \nfocussearch\n infill optimization function is suitable.\nIf we use \nmean\n as infill criterion any kind of model which can handle with factor variables can be used here (like random tree, random forest, linear model and many others).\n\n\nmbo2 = mbo(objfun2, design = design2, learner = learner_rf, control = control2, show.info = FALSE)\nmbo2$y\n\n\n\n\nIn contrast, if one will apply \nei\n or \ncb\n infill criteria,\nthe \npredict.type\n attribute of the learner have be set to \nse\n, if possible. A list of regression learners which support it can be viewed by:\n\n\n#listLearners(type = \nregr\n, se = TRUE)\n\n\n\n\n\n\n\nWe hence modify the random forest learner and optimize \nobjfun2\n by \nei\n infill criterion.\n\n\nlearner_rf = makeLearner(\nregr.randomForest\n, predict.type = \nse\n)\ncontrol2$infill.crit = \nei\n\nmbo2 = mbo(objfun2, design = design2, learner = learner_rf, control = control2, show.info = FALSE)\nmbo2$y\n\n\n\n\nFinally, if a learner, which does not support the \nse\n prediction type, should be applied for the optimization with \nei\n infill criterion, there is a possibility to create a bagging model with the desired characteristics. \nThe attribute \nbw.iters\n provides the number of models in the ensemble, see documentation for \nmakeBaggingWrapper\n of \nmlr\n package.\n\n\nlearner_rt = makeLearner(\nregr.rpart\n)\nbag_rt = makeBaggingWrapper(learner_rt, bw.iters = 5)\nbag_rt = setPredictType(bag_rt, predict.type = \nse\n)\nmbo2 = mbo(objfun2, design = design2, learner = bag_rt, control = control2, show.info = FALSE)\nmbo2$y", 
            "title": "In Depth Introduction"
        }, 
        {
            "location": "/in_depth_introduction/index.html#objective-function", 
            "text": "The first argument of  mbo()  is the name of the objective function created with the package  smoof . \nThe first argument of this objective function has to be a list of values.\nThe function has to return a single numerical value. \nWe demonstrate in this tutorial the optimization of two simple functions: The 5 dimensional  ackley function  ( objfun1 ) and a self-constructed sine und cosine combination ( objfun2 ), where we do not minimize but maximize the function.  objfun1  depends on 5 numeric parameters, while  objfun2  assumes 2 numeric and 1 discrete parameters.  The 5 dimensional  ackley function  can be get by using the appropriate function of the  smoof  package  objfun1 = makeAckleyFunction(5)   The self-constructed function is built with  makeSingleObjetiveFunction . \nIt has the attribute  par.set  that has to be a ParamSet object from the  ParamHelpers  package, which provides information about the parameters of the objective function and their constraints for optimization.\nWe assume  j  from interval [0,1] and  k  from interval [1,2]. Parameter  method  can be either  \"a\"  or  \"b\" .\nThe type of optimization, in this case maximization, is also defined in  makeSingleObjetiveFunction  with  minimize = TRUE  as default.  foo = function(x) {\n  j = x[[1]]\n  k = x[[2]]\n  method = x[[3]]\n  perf = ifelse(method ==  a , k * sin(j) + cos(j),\n               sin(j) + k * cos(j))\n  return(perf)\n}\nobjfun2 = makeSingleObjectiveFunction(\n  name =  example ,\n  fn = foo,\n  par.set = makeParamSet(\n    makeNumericParam( j , lower = 0,upper = 1),\n    makeIntegerParam( k , lower = 1, upper = 2),\n    makeDiscreteParam( method , values = c( a ,  b ))\n  ),\n  has.simple.signature = FALSE,\n  minimize = FALSE\n)", 
            "title": "Objective Function"
        }, 
        {
            "location": "/in_depth_introduction/index.html#initial-design", 
            "text": "The second argument of the  mbo()  function -  design  - is the initial design with default setting  NULL .  It is recommendable to use the  generateDesign  function from  ParamHelpers  package to create it.\nHowever, if special designs are desired (e.g., orthogonal designs), its interface has to be the same as the interface of the  generateDesign  objects. \nParticular attention has to be paid to the setting of the  trafo  attribute.  init.points1 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun1)))\n# NOTE:\n# init.points1 = 5 * getParamLengths(getParamSet(objfun1))\n# is not working, because mlr::getParamSet() is used by default.\ninit.fun1 = randomLHS\nset.seed(1)\ndesign1 = generateDesign(n = init.points1, par.set = smoof::getParamSet(objfun1), fun = init.fun1, trafo = FALSE)\n\ninit.points2 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun2)))\ninit.fun2 = maximinLHS\ninit.args2 = list(k = 3, dup = 4)\ndesign2 = generateDesign(n = init.points2, par.set = smoof::getParamSet(objfun2), fun.args = init.args2, fun = init.fun2, trafo = FALSE)", 
            "title": "Initial Design"
        }, 
        {
            "location": "/in_depth_introduction/index.html#surrogate-model", 
            "text": "Attribute  learner  of the  mbo()  function allows us to choose an appropriate surrogate model for the parameter optimization.\nIt can be easily done using the  makeLearner  function from  mlr  package.\nList of implemented learners can be seen using the ?learners command or on  http://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/  \nThe choice of the surrogate model depends on the parameter set of the objective function.\nWhile kriging models are advisable for the numeric parameters, random forest models can be used if at least one parameter is factorial.\nIn our example we consider these two surrogate models: kriging  for optimizing of  objfun1   and  random forest  for  objfun2 .  learner_km = makeLearner( regr.km , predict.type =  se , covtype =  matern3_2 )\nlearner_rf = makeLearner( regr.randomForest )  However, in some cases it is necessary to modify the learners (e.g., in order to get the standard error prediction for design points).\nThis will be discussed und illustrated in the section \"Experiments and Output\".", 
            "title": "Surrogate Model"
        }, 
        {
            "location": "/in_depth_introduction/index.html#mbocontrol", 
            "text": "The  MBOControl  object can be created with  makeMBOControl  and some attributes can be set when creating it.\nBut there are also further functions to define or change the settings of the object:", 
            "title": "MBOControl"
        }, 
        {
            "location": "/in_depth_introduction/index.html#setmbocontrolinfill", 
            "text": "", 
            "title": "setMBOControlInfill"
        }, 
        {
            "location": "/in_depth_introduction/index.html#attribute-crit", 
            "text": "One of the most important issues is to define how the next design points in the sequential loops have to be chosen. \nFirstly, we have to choose the infill criterion using the   crit  attribute of  setMBOControlInfill .\nAt the moment five possibilities are implemented:   mean : mean response of the surrogate model,   ei : expected improvement of the surrogate model,   aei : augmented expected improvement, which is especially useful for the noisy functions,   eqi : expected quantile improvement\n*  cb : confidence bound which is the additive combination of mean response and mean standard error estimation of the surrogate model (response - lambda * standard.error). \nThe default value of lambda is 1, but it can easily be changed by\nthe  crit.cb.lambda  attribute.", 
            "title": "Attribute crit"
        }, 
        {
            "location": "/in_depth_introduction/index.html#attribute-opt", 
            "text": "The attribute  opt  sets how the next point to evaluate should be proposed given an infill criterion. \nThe possibilities are  focussearch ,  cmaes ,  ea  and  nsga2 .  If  focussearch  is chosen, the common procedure is as follows: \nin the first step an lhs design is sampled in the parameter space (by  randomLHS  function) and the design point with the best  prediction of the infill criterion is determined. \nUser can set the size of this design by the  opt.focussearch.points  attribute (default value is 10000). \nIn the second step the parameter space is shrunk around the best design point in a certain way which should not be discussed in detail here. \nFirst and second steps are repeated iteratively  opt.focussearch.maxit  times (default is 5) while the best seen value of the infill criterion is passed back.  If  opt  is  cmaes , the point, which optimizes the\ninfill criterion, is chosen via  cma_es  function of   cmaes  package. Control argument for  cmaes  optimizer can be provided in\nthe  opt.cmaes.control  attribute (default is empty list).  If  opt  is  ea  a simple (mu+1)-evolutionary optimization algorithm is used to optimize the infill criterion.\nThe population size mu can be set by the  opt.ea.mu  attribute (default value is 10).\n(mu+1) means that in each population only one child is generated using crossover und mutation operators (from  emao  package).  The parameters  eta  and  p  of the latter two operators can be adjusted via the attributes  opt.ea.sbx.eta ,  opt.ea.sbx.p , opt.ea.pm.eta  and  opt.ea.pm.p .\nThe default number of EA iterations is 500 and can be changed by  opt.ea.maxit  attribute.  For multi objective optimization,  opt  should be set to  nsga2 . \nThis is needed for mspot.  As all four infill optimization strategies do not guarantee  to find the global optimum, users can set the number of restarts by the  opt.restarts  attribute (default value is 1).\nAfter conducting the desired number of restarts the design point with the best infill criterion value is passed back to the MBO loop.  Please note that just  focussearch  optimizer is suitable for the case of factor parameters in the parameter set!", 
            "title": "Attribute opt"
        }, 
        {
            "location": "/in_depth_introduction/index.html#further-functions", 
            "text": "The number of sequential steps (iterations) can be set via the attribute  iters  of the function  setMBOControlTermination  (default setting is 10).  There are also many other functions and attributes which user can set in a desired way...  \nThe list of all attributes is provided in the software documentation.  control1 = makeMBOControl()\ncontrol1 = setMBOControlInfill(\n  control = control1,\n  crit =  ei ,\n  opt =  cmaes \n)\ncontrol1 = setMBOControlTermination(\n  control = control1,\n  iters = 10\n)\n\ncontrol2 = makeMBOControl()\ncontrol2 = setMBOControlInfill(\n  control = control2,\n  crit =  mean ,\n  opt =  focussearch \n)\ncontrol2 = setMBOControlTermination(\n  control = control2,\n  iters = 10\n)", 
            "title": "Further functions"
        }, 
        {
            "location": "/in_depth_introduction/index.html#experiments-and-output", 
            "text": "Now we will apply the mbo() function to optimize the two objective functions.", 
            "title": "Experiments and Output"
        }, 
        {
            "location": "/in_depth_introduction/index.html#optimization-of-objfun1", 
            "text": "mbo1 = mbo(objfun1, design = design1, learner = learner_km, control = control1, show.info = TRUE)\nmbo1\ngetOptPathY(mbo1$opt.path,  y ) # get all y values  The output of mbo function is a structure of several variables. The most important are:   x: the best point of the parameter space  y: the associated best value of the objective function  opt.path: optimization path. See  ParamHelpers  for further information.  models: If no other setting were provided in the  MBOControl  object, the last estimated surrogate is given here.  ...    We can also change some attributes of the  MBOControl  object and run mbo() function again  control1$infill.crit =  mean \ncontrol1$infill.opt =  focussearch \nmbo1 = mbo(objfun1, design = design1, learner = learner_km, control = control1, show.info = FALSE)\nmbo1$y", 
            "title": "Optimization of objfun1"
        }, 
        {
            "location": "/in_depth_introduction/index.html#optimization-of-objfun2", 
            "text": "Let us apply  mlrMBO  package to optimize object2 function, which contains one factor variable.\nWe have already mentioned before that in this case just the  focussearch  infill optimization function is suitable.\nIf we use  mean  as infill criterion any kind of model which can handle with factor variables can be used here (like random tree, random forest, linear model and many others).  mbo2 = mbo(objfun2, design = design2, learner = learner_rf, control = control2, show.info = FALSE)\nmbo2$y  In contrast, if one will apply  ei  or  cb  infill criteria,\nthe  predict.type  attribute of the learner have be set to  se , if possible. A list of regression learners which support it can be viewed by:  #listLearners(type =  regr , se = TRUE)   We hence modify the random forest learner and optimize  objfun2  by  ei  infill criterion.  learner_rf = makeLearner( regr.randomForest , predict.type =  se )\ncontrol2$infill.crit =  ei \nmbo2 = mbo(objfun2, design = design2, learner = learner_rf, control = control2, show.info = FALSE)\nmbo2$y  Finally, if a learner, which does not support the  se  prediction type, should be applied for the optimization with  ei  infill criterion, there is a possibility to create a bagging model with the desired characteristics. \nThe attribute  bw.iters  provides the number of models in the ensemble, see documentation for  makeBaggingWrapper  of  mlr  package.  learner_rt = makeLearner( regr.rpart )\nbag_rt = makeBaggingWrapper(learner_rt, bw.iters = 5)\nbag_rt = setPredictType(bag_rt, predict.type =  se )\nmbo2 = mbo(objfun2, design = design2, learner = bag_rt, control = control2, show.info = FALSE)\nmbo2$y", 
            "title": "Optimization of objfun2"
        }, 
        {
            "location": "/parallelization/index.html", 
            "text": "Parallelization\n\n\nIt is possible to parallelize the evaluation of the target function to speed up the computation. Internally the\nevaluation of the target function is realized with the R package parallelMap. This package offers a parallel version\nof the \nlapply\n function and offers a lot of modes. We cannot review all of them here. The \nparallelMap github page\n offers a nice tutorial, describes all modes thorougly and\ninforms you about the available options. Here we just provide a simple example on how to take advantage of multiple cores on a single machine.\n\n\nlibrary(\nmlrMBO\n)\nlibrary(\nparallelMap\n)\n\nobj.fun = makeSphereFunction(1)\nlearner = makeLearner(\nregr.km\n, predict.type = \nse\n, covtype = \nmatern3_2\n)\ncontrol = makeMBOControl()\ncontrol = setMBOControlTermination(control, iters = 3)\ndesign = generateDesign(n = 6, par.set = smoof::getParamSet(obj.fun))\n\nparallelStartMulticore(cpus = 2) # use 2 CPUs\nres = mbo(obj.fun, par.set, learner = learner, control = control)\nparallelStop()", 
            "title": "Parallelization"
        }, 
        {
            "location": "/parallelization/index.html#parallelization", 
            "text": "It is possible to parallelize the evaluation of the target function to speed up the computation. Internally the\nevaluation of the target function is realized with the R package parallelMap. This package offers a parallel version\nof the  lapply  function and offers a lot of modes. We cannot review all of them here. The  parallelMap github page  offers a nice tutorial, describes all modes thorougly and\ninforms you about the available options. Here we just provide a simple example on how to take advantage of multiple cores on a single machine.  library( mlrMBO )\nlibrary( parallelMap )\n\nobj.fun = makeSphereFunction(1)\nlearner = makeLearner( regr.km , predict.type =  se , covtype =  matern3_2 )\ncontrol = makeMBOControl()\ncontrol = setMBOControlTermination(control, iters = 3)\ndesign = generateDesign(n = 6, par.set = smoof::getParamSet(obj.fun))\n\nparallelStartMulticore(cpus = 2) # use 2 CPUs\nres = mbo(obj.fun, par.set, learner = learner, control = control)\nparallelStop()", 
            "title": "Parallelization"
        }
    ]
}